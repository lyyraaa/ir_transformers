{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-09T13:58:47.767399500Z",
     "start_time": "2025-04-09T13:58:37.974314700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import openpyxl\n",
    "import h5py\n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.transforms import v2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.decomposition import PCA\n",
    "import sys\n",
    "import os\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Experiment Hyperparameters"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5c3fa5c69dc9bda7"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "is_local = True # todo\n",
    "\n",
    "# Experiment\n",
    "seed = 0 if is_local else int(sys.argv[-2])\n",
    "torch.manual_seed(seed)\n",
    "image_size = 256\n",
    "\n",
    "# Data: which wavenumbers are even allowed to be considered?\n",
    "wv_start = 0\n",
    "wv_end = 965\n",
    "\n",
    "# Data loading\n",
    "test_set_fraction = 0.2\n",
    "val_set_fraction = 0.2\n",
    "batch_size= 2 # todo see how high can get on csf\n",
    "use_augmentation = True\n",
    "\n",
    "# Network\n",
    "dropout_p=0\n",
    "\n",
    "# Training schedule\n",
    "lr = 1e-5\n",
    "l2 = 5e-2\n",
    "max_epochs=200\n",
    "\n",
    "# dimensionality reduction parameters\n",
    "r_method = 'linear' # {'linear','pca,'fixed'}\n",
    "reduce_dim = 16 if is_local else int(sys.argv[-1]) \n",
    "channels_used = np.s_[...,wv_start:wv_end] # used only when r_method = 'fixed'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-09T13:58:47.825753200Z",
     "start_time": "2025-04-09T13:58:47.767399500Z"
    }
   },
   "id": "222959fcfa561aff",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 228 cores\n",
      "Using 965/965 wavenumbers\n"
     ]
    }
   ],
   "source": [
    "def csf_fp(filepath):\n",
    "    return filepath.replace('D:/datasets','D:/datasets' if is_local else './')\n",
    "\n",
    "master = pd.read_excel(csf_fp(rf'D:/datasets/pcuk2023_ftir_whole_core/master_sheet.xlsx'))\n",
    "slide = master['slide'].to_numpy()\n",
    "patient_id = master['patient_id'].to_numpy()\n",
    "hdf5_filepaths = np.array([csf_fp(fp) for fp in master['hdf5_filepath']])\n",
    "annotation_filepaths = np.array([csf_fp(fp) for fp in master['annotation_filepath']])\n",
    "mask_filepaths = np.array([csf_fp(fp) for fp in master['mask_filepath']])\n",
    "wavenumbers = np.load(csf_fp(f'D:/datasets/pcuk2023_ftir_whole_core/wavenumbers.npy'))[wv_start:wv_end]\n",
    "wavenumbers_used = wavenumbers[channels_used]\n",
    "\n",
    "annotation_class_colors = np.array([[0,255,0],[128,0,128],[255,0,255],[0,0,255],[255,165,0],[255,0,0]])\n",
    "annotation_class_names = np.array(['epithelium_n','stroma_n','epithelium_c','stroma_c','corpora_amylacea','blood'])\n",
    "n_classes = len(annotation_class_names)\n",
    "print(f\"Loaded {len(slide)} cores\")\n",
    "print(f\"Using {len(wavenumbers_used)}/{len(wavenumbers)} wavenumbers\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-09T13:58:47.880822100Z",
     "start_time": "2025-04-09T13:58:47.828766400Z"
    }
   },
   "id": "a78af96389a4cd31",
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define Datasets, Dataloaders"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aaba53c2e93ca461"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patients per data split:\n",
      "\tTRAIN: 135\n",
      "\tVAL: 49\n",
      "\tTEST: 44\n"
     ]
    }
   ],
   "source": [
    "unique_pids = np.unique(patient_id)\n",
    "pids_trainval, pids_test, _, _ = train_test_split(\n",
    "    unique_pids, np.zeros_like(unique_pids), test_size=test_set_fraction, random_state=seed)\n",
    "pids_train, pids_val, _, _ = train_test_split(\n",
    "    pids_trainval, np.zeros_like(pids_trainval), test_size=(val_set_fraction/(1-test_set_fraction)), random_state=seed)\n",
    "where_train = np.where(np.isin(patient_id,pids_train))\n",
    "where_val = np.where(np.isin(patient_id,pids_val))\n",
    "where_test = np.where(np.isin(patient_id,pids_test))\n",
    "print(f\"Patients per data split:\\n\\tTRAIN: {len(where_train[0])}\\n\\tVAL: {len(where_val[0])}\\n\\tTEST: {len(where_test[0])}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-09T13:58:47.885529700Z",
     "start_time": "2025-04-09T13:58:47.879796700Z"
    }
   },
   "id": "e4655cf38851b265",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class ftir_annot_dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, \n",
    "                 hdf5_filepaths, mask_filepaths, annotation_filepaths, channels_use, augment=False):\n",
    "        self.hdf5_filepaths = hdf5_filepaths\n",
    "        self.mask_filepaths = mask_filepaths\n",
    "        self.annotation_filepaths = annotation_filepaths\n",
    "        self.channels_use = channels_use\n",
    "        self.augment=augment\n",
    "        \n",
    "        # class data\n",
    "        self.annotation_class_colors = np.array([[0,255,0],[128,0,128],[255,0,255],[0,0,255],[255,165,0],[255,0,0]])\n",
    "        self.annotation_class_names = np.array(['epithelium_n','stroma_n','epithelium_c','stroma_c','corpora_amylacea','blood'])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.hdf5_filepaths)\n",
    "    \n",
    "    # split annotations from H x W x 3 to C x H x W, one/zerohot along C dimension\n",
    "    def split_annotations(self,annotations_img):\n",
    "        split = torch.zeros((len(self.annotation_class_colors),*annotations_img.shape[:-1]))\n",
    "        for c,col in enumerate(annotation_class_colors):\n",
    "            split[c,:,:] = torch.from_numpy(np.all(annotations_img == self.annotation_class_colors[c],axis=-1)) \n",
    "        return split\n",
    "        \n",
    "    def __getitem__(self, idx):    \n",
    "        \n",
    "        # open hdf5 file\n",
    "        hdf5_file = h5py.File(self.hdf5_filepaths[idx],'r')\n",
    "        \n",
    "        # get mask\n",
    "        mask = torch.from_numpy(\n",
    "            hdf5_file['mask'][:],\n",
    "        ).unsqueeze(0)\n",
    "        \n",
    "        # get ftir\n",
    "        ftir = torch.from_numpy(\n",
    "            hdf5_file['spectra'][*self.channels_use],\n",
    "        ).permute(2,0,1)\n",
    "        hdf5_file.close()\n",
    "        ftir *= mask\n",
    "        \n",
    "        # get annotations\n",
    "        annotations = self.split_annotations(cv2.imread(self.annotation_filepaths[idx])[:,:,::-1])\n",
    "        annotations *= mask\n",
    "        has_annotations = annotations.sum(dim=0) != 0\n",
    "        \n",
    "        if self.augment:\n",
    "            to_aug = torch.rand((2,))\n",
    "            if to_aug[0] > 0.5: #hflip\n",
    "                ftir = torch.flip(ftir, (-1,))\n",
    "                annotations = torch.flip(annotations, (-1,))\n",
    "                has_annotations = torch.flip(has_annotations, (-1,))\n",
    "                mask = torch.flip(mask, (-1,))\n",
    "            if to_aug[1] > 0.5: #vflip\n",
    "                ftir = torch.flip(ftir, (-2,))\n",
    "                annotations = torch.flip(annotations, (-2,))\n",
    "                has_annotations = torch.flip(has_annotations, (-2,))\n",
    "                mask = torch.flip(mask, (-2,))\n",
    "        \n",
    "        return ftir, annotations, mask, has_annotations"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-09T13:58:47.890086800Z",
     "start_time": "2025-04-09T13:58:47.887564700Z"
    }
   },
   "id": "a8a3aa59fbf57012",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loader sizes:\n",
      "\ttrain: 67\n",
      "\tval: 24\n",
      "\ttest: 22\n"
     ]
    }
   ],
   "source": [
    "dataset_train = ftir_annot_dataset(\n",
    "    hdf5_filepaths[where_train], mask_filepaths[where_train], annotation_filepaths[where_train], channels_used, augment=use_augmentation,\n",
    ")\n",
    "dataset_val = ftir_annot_dataset(\n",
    "    hdf5_filepaths[where_val], mask_filepaths[where_val], annotation_filepaths[where_val], channels_used, augment=False,\n",
    ")\n",
    "dataset_test = ftir_annot_dataset(\n",
    "    hdf5_filepaths[where_test], mask_filepaths[where_test], annotation_filepaths[where_test], channels_used, augment=False,\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size, shuffle=True,drop_last=True)\n",
    "val_loader = torch.utils.data.DataLoader(dataset_val, batch_size=batch_size, shuffle=True,drop_last=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset_test, batch_size=batch_size,shuffle=True,drop_last=True)\n",
    "print(f\"loader sizes:\\n\\ttrain: {len(train_loader)}\\n\\tval: {len(val_loader)}\\n\\ttest: {len(test_loader)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-09T13:58:47.895664500Z",
     "start_time": "2025-04-09T13:58:47.892154800Z"
    }
   },
   "id": "c6bebd9eeed34711",
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define dimensionality reduction method"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "27b5434becc5286e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class LinearReduction(nn.Module):\n",
    "    def __init__(self,input_dim,reduce_dim):\n",
    "        super().__init__()\n",
    "        self.reduce_dim = reduce_dim\n",
    "        self.input_norm = nn.BatchNorm2d(input_dim)\n",
    "        self.projection = nn.Conv2d(input_dim,reduce_dim,kernel_size=1,stride=1)\n",
    "        self.projection_norm = nn.BatchNorm2d(reduce_dim)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self.projection_norm(self.projection(self.input_norm(x)))\n",
    "    \n",
    "class PCAReduce(nn.Module):\n",
    "    def __init__(self,reduce_dim,means,loadings):\n",
    "        super().__init__()\n",
    "        self.reduce_dim = reduce_dim\n",
    "        self.register_buffer('means', torch.from_numpy(means).float().reshape(1,-1,1,1))\n",
    "        self.register_buffer('loadings', torch.from_numpy(loadings).float())\n",
    "    \n",
    "    def forward(self,x):\n",
    "        projected = x - self.means\n",
    "        \n",
    "        b,c,h,w = projected.shape\n",
    "        projected = projected.permute(0,2,3,1).reshape(b,h*w,c)\n",
    "        projected = torch.matmul(projected, self.loadings.T)\n",
    "        projected = projected.reshape(b,h,w,self.reduce_dim).permute(0,3,1,2)\n",
    "        \n",
    "        return projected\n",
    "        \n",
    "class FixedReduction(nn.Module):\n",
    "    def __init__(self,input_dim):\n",
    "        super().__init__()\n",
    "        self.input_norm = nn.BatchNorm2d(input_dim)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self.input_norm(x)\n",
    "\n",
    "if r_method == 'pca':\n",
    "    spectral_sample = []\n",
    "    batch_samples = 0\n",
    "    for data, annotations, mask, has_annotations in train_loader:\n",
    "        where = torch.where(has_annotations[0] == 1)\n",
    "        ridxs = torch.randperm(where[0].shape[0])[:100]\n",
    "        spectral_sample.append(data[:, :, where[0][ridxs],where[1][ridxs]].permute(0,2,1).flatten(0,1).numpy())\n",
    "        batch_samples += 1\n",
    "        if batch_samples > 10: break\n",
    "    spectral_sample = np.concatenate(spectral_sample,axis=0)\n",
    "    spectral_means = np.mean(spectral_sample,axis=0)\n",
    "    spectral_sample -= spectral_means\n",
    "    pca = PCA(n_components=reduce_dim)\n",
    "    pca.fit(spectral_sample)\n",
    "    spectral_loadings = pca.components_"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-09T13:58:47.922689400Z",
     "start_time": "2025-04-09T13:58:47.897682700Z"
    }
   },
   "id": "b5f1cfe137e09f85",
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define Model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f550708e9f4c74c5"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torchvision\n",
    "class SegViT(torchvision.models.VisionTransformer):\n",
    "    def __init__(self,*args,**kwargs):\n",
    "        super().__init__(*args,**kwargs)\n",
    "        if r_method == 'pca':\n",
    "            self.input_processing = PCAReduce(reduce_dim,spectral_means,spectral_loadings)\n",
    "        elif r_method == 'fixed':\n",
    "            self.input_processing = FixedReduction(input_dim=len(wavenumbers_used))\n",
    "        elif r_method == 'linear':\n",
    "            self.input_processing = LinearReduction(input_dim=len(wavenumbers_used),reduce_dim=reduce_dim)\n",
    "            \n",
    "        self.conv_proj = nn.Conv2d(\n",
    "            in_channels=len(wavenumbers_used) if r_method == 'fixed' else reduce_dim, \n",
    "            out_channels=self.hidden_dim,\n",
    "            kernel_size=self.patch_size,\n",
    "            stride=self.patch_size\n",
    "        )\n",
    "            \n",
    "        self.upscale = nn.ConvTranspose2d(self.hidden_dim,self.hidden_dim,kernel_size=8,stride=8)\n",
    "        self.heads = nn.Sequential(\n",
    "            nn.Conv2d(self.hidden_dim + (len(wavenumbers_used) if r_method == 'fixed' else reduce_dim),self.hidden_dim//2,kernel_size=3,stride=1,padding=1,bias=False),\n",
    "            nn.BatchNorm2d(self.hidden_dim//2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(self.hidden_dim//2,self.hidden_dim//4,kernel_size=3,stride=1,padding=1,bias=False),\n",
    "            nn.BatchNorm2d(self.hidden_dim//4),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(self.hidden_dim//4,n_classes,kernel_size=3,stride=1,padding=1),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        b,c,h,w = x.shape\n",
    "        hpatch,wpatch = h//self.patch_size,w//self.patch_size\n",
    "        \n",
    "        # Reshape and permute the input tensor\n",
    "        x_in = self.input_processing(x)\n",
    "        x = self._process_input(x_in)\n",
    "        n = x.shape[0]\n",
    "    \n",
    "        # Expand the class token to the full batch\n",
    "        batch_class_token = self.class_token.expand(n, -1, -1)\n",
    "        x = torch.cat([batch_class_token, x], dim=1)\n",
    "    \n",
    "        x = self.encoder(x)\n",
    "    \n",
    "        # Classifier \"token\" as used by standard language architectures\n",
    "        # B x hpatch*wpatch x self.hidden_dim\n",
    "        x = x[:, 1:].permute(0,2,1).reshape(b,self.hidden_dim,hpatch,wpatch)\n",
    "        x = self.upscale(x)\n",
    "        x = self.heads(torch.cat([x,x_in],dim=1))\n",
    "    \n",
    "        return x\n",
    "    \n",
    "model = SegViT(\n",
    "    image_size=256,\n",
    "    patch_size=8,\n",
    "    num_layers=12,\n",
    "    num_heads=12,\n",
    "    hidden_dim=192,\n",
    "    num_classes=n_classes,\n",
    "    mlp_dim=192*4,\n",
    ")\n",
    "\n",
    "model = model.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-09T13:58:47.952710Z",
     "start_time": "2025-04-09T13:58:47.904967800Z"
    }
   },
   "id": "33535f38f1458959",
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training Loop"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4dfdaf2ad556fa46"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss(reduction='none')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr,weight_decay=l2)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=15, threshold=0.01, cooldown=0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-09T13:58:47.958211800Z",
     "start_time": "2025-04-09T13:58:47.953743700Z"
    }
   },
   "id": "50a59f2ddf1e3f0d",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "training_losses,validation_losses = [],[]\n",
    "training_accs,validation_accs = [],[]\n",
    "training_f1ms,validation_f1ms = [],[]\n",
    "training_f1s,validation_f1s = [],[]\n",
    "lr_decreases = []\n",
    "current_iters = 0\n",
    "best_val_f1 = 0\n",
    "best_val_iter = 0\n",
    "stop_training=False"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-09T13:58:47.962472Z",
     "start_time": "2025-04-09T13:58:47.958831900Z"
    }
   },
   "id": "51f82808428ba870",
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ✰ ✰ ✰ EPOCH 1 ✰ ✰ ✰ \n",
      "train : \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(max_epochs):\n",
    "    print(f\"\\n ✰ ✰ ✰ EPOCH {epoch+1} ✰ ✰ ✰ \")\n",
    "    \n",
    "    # reset running metrics\n",
    "    running_loss_train, running_loss_val = 0, 0\n",
    "    train_preds,train_targets = [],[]\n",
    "    val_preds,val_targets = [],[]\n",
    "    \n",
    "    # Train loop\n",
    "    model.train()\n",
    "    batch_frac = 42 / (len(train_loader))\n",
    "    for batch_idx, (data, annot, mask, has_annotations) in enumerate(train_loader):\n",
    "        print(f\"train : {'█'*int(batch_idx*batch_frac)}\", end=\"\\r\")\n",
    "        \n",
    "        # Put data and label on device\n",
    "        data = data.to(device); annot = annot.to(device); has_annotations = has_annotations.to(device)\n",
    "        \n",
    "        # Push data through model\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = loss_fn(out,annot.argmax(dim=1)) * has_annotations # loss per pixel\n",
    "        loss = loss.sum() / (has_annotations.sum()) # mean loss per annotated pixel\n",
    "        loss.backward() # backprop\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Calculate metrics\n",
    "        running_loss_train += loss.cpu().item()\n",
    "        targets = annot.argmax(dim=1)[has_annotations] # class targets on annotated pixels\n",
    "        preds = out.argmax(dim=1)[has_annotations] # predicted values on annotated pixels\n",
    "        train_preds.extend(preds.detach().cpu().numpy())\n",
    "        train_targets.extend(targets.detach().cpu().numpy())\n",
    "    print(f\"train : {'█'*42}\")\n",
    "        \n",
    "    # Validate loop\n",
    "    model.eval()\n",
    "    batch_frac = 42 / len(val_loader)\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, annot, mask, has_annotations) in enumerate(val_loader):\n",
    "            print(f\"val   : {'█'*int(batch_idx*batch_frac)}\", end=\"\\r\")\n",
    "            \n",
    "            # Put data and label on device\n",
    "            data = data.to(device); annot = annot.to(device); has_annotations = has_annotations.to(device)\n",
    "            \n",
    "            # Push data through model\n",
    "            out = model(data)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = loss_fn(out,annot.argmax(dim=1)) * has_annotations # loss per pixel\n",
    "            loss = loss.sum() / (has_annotations.sum()) # mean loss per annotated pixel\n",
    "            \n",
    "            # Calculate metrics\n",
    "            running_loss_val += loss.cpu().item()\n",
    "            targets = annot.argmax(dim=1)[has_annotations] # class targets on annotated pixels\n",
    "            preds = out.argmax(dim=1)[has_annotations] # predicted values on annotated pixels\n",
    "            val_preds.extend(preds.detach().cpu().numpy())\n",
    "            val_targets.extend(targets.detach().cpu().numpy())\n",
    "    print(f\"val   : {'█'*42}\")\n",
    "    \n",
    "    # calculate epoch metrics for train set\n",
    "    train_acc = accuracy_score(train_targets, train_preds); training_accs.append(train_acc)\n",
    "    train_f1m = f1_score(train_targets, train_preds, average='macro'); training_f1ms.append(train_f1m)\n",
    "    train_f1 = f1_score(train_targets, train_preds, average=None); training_f1s.append(train_f1)\n",
    "    train_loss = running_loss_train / (len(dataset_train)); training_losses.append(train_loss)\n",
    "    \n",
    "    # calculate epoch metrics for val set\n",
    "    val_acc = accuracy_score(val_targets, val_preds); validation_accs.append(val_acc)\n",
    "    val_f1m = f1_score(val_targets, val_preds, average='macro'); validation_f1ms.append(val_f1m)\n",
    "    val_f1 = f1_score(val_targets, val_preds, average=None); validation_f1s.append(val_f1)\n",
    "    val_loss = running_loss_val / (len(dataset_val)); validation_losses.append(val_loss)\n",
    "    \n",
    "    # Update\n",
    "    print(f\"TRAIN --- | Loss: {train_loss:.4} | OA: {train_acc:.4} | f1: {train_f1m:.4}\")\n",
    "    print(f\"VAL ----- | Loss: {val_loss:.4} | OA: {val_acc:.4} | f1: {val_f1m:.4}\")\n",
    "    \n",
    "    scheduler.step(val_f1m)\n",
    "    new_lr = optimizer.param_groups[0]['lr']\n",
    "    if new_lr != lr:\n",
    "        print(f\"Val f1 plateaued, lr {lr} -> {new_lr}\")\n",
    "        lr = new_lr\n",
    "        lr_decreases.append(epoch)\n",
    "        if len(lr_decreases) >= 3: \n",
    "            print(\"Val f1 decreased thrice, ending training early\")\n",
    "            break\n",
    "\n",
    "    if val_f1m > best_val_f1:\n",
    "        best_val_f1 = val_f1m\n",
    "        best_val_epoch = epoch\n",
    "        if not is_local:\n",
    "            torch.save(model.state_dict(), rf'./model_weights_{seed}.pt')\n",
    "\n",
    "if not is_local:\n",
    "    model.load_state_dict(torch.load(rf'./model_weights_{seed}.pt', weights_only=True))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-09T13:58:56.165635Z",
     "start_time": "2025-04-09T13:58:47.966572500Z"
    }
   },
   "id": "7c6adbd14fd799f6",
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test Loop"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "307cf693fb06cb7e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Test\n",
    "running_loss_test = 0\n",
    "test_preds, test_targets = [], []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (data, annot, mask, has_annotations) in enumerate(test_loader):\n",
    "        # Put data and label on device\n",
    "        data = data.to(device); annot = annot.to(device); has_annotations = has_annotations.to(device)\n",
    "        \n",
    "        # Push data through model\n",
    "        out = model(data)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = loss_fn(out,annot.argmax(dim=1)) * has_annotations # loss per pixel\n",
    "        loss = loss.sum() / (has_annotations.sum()) # mean loss per annotated pixel\n",
    "        \n",
    "        # Calculate metrics\n",
    "        running_loss_test += loss.cpu().item()\n",
    "        targets = annot.argmax(dim=1)[has_annotations] # class targets on annotated pixels\n",
    "        preds = out.argmax(dim=1)[has_annotations] # predicted values on annotated pixels\n",
    "        test_preds.extend(preds.detach().cpu().numpy())\n",
    "        test_targets.extend(targets.detach().cpu().numpy())\n",
    "        \n",
    "        # Save pred figure todo remove\n",
    "        if is_local:\n",
    "            for b in range(data.shape[0]):\n",
    "                bidx = (batch_idx*1) + b\n",
    "                fig, ax = plt.subplots(figsize=(8, 4))\n",
    "                predcore = annotation_class_colors[out[b].argmax(dim=0).detach().cpu().numpy()].reshape(256,256,3) / 255\n",
    "                annotcolour = annotation_class_colors[annot[b].argmax(dim=0).cpu().numpy()] / 255\n",
    "                predcore *= mask[b,0].cpu().numpy()[...,np.newaxis]\n",
    "                annotcolour *= has_annotations[b].cpu().numpy()[...,np.newaxis]\n",
    "                annotcolour += mask[b,0].cpu().numpy()[...,np.newaxis] * 1 - has_annotations[b].cpu().numpy()[...,np.newaxis]\n",
    "                ax.imshow(np.hstack([predcore,annotcolour]))\n",
    "                ax.set_axis_off()\n",
    "                #ax.text(235, 220, dataset_test.hdf5_filepaths[bidx].split('/')[-1][:-3], fontsize=12, color='cyan', fontweight='bold')\n",
    "                fig.tight_layout()\n",
    "\n",
    "# calculate test set metrics\n",
    "test_acc = accuracy_score(test_targets, test_preds)\n",
    "test_f1m = f1_score(test_targets, test_preds, average='macro')\n",
    "test_f1 = f1_score(test_targets, test_preds, average=None)\n",
    "test_loss = running_loss_test / batch_idx\n",
    "\n",
    "print(f\"TEST ---- | Loss: {test_loss:.4} | OA: {test_acc:.4} | f1: {test_f1m:.4}\")\n",
    "for cls_idx, f1 in enumerate(test_f1):\n",
    "    print(f\"{annotation_class_names[cls_idx]}{(20 - len(annotation_class_names[cls_idx])) * ' '} : {f1:.4}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-04-09T13:58:56.167661200Z"
    }
   },
   "id": "242a34c1a6fe3be8",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "67ff17a2ebecf4ae"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,3,figsize=(16,5))\n",
    "ax[0].plot(np.arange(1,len(training_losses)+1),np.array(training_losses),color='cornflowerblue',label=\"train\")\n",
    "ax[0].plot(np.arange(1,len(validation_losses)+1),np.array(validation_losses),color='orange',label=\"validation\")\n",
    "ax[0].scatter(len(validation_losses),test_loss,color='green',label=\"test\",marker=\"x\")\n",
    "ax[0].set_title(\"loss curves\"); ax[0].legend()\n",
    "\n",
    "ax[1].plot(np.arange(1,len(training_accs)+1),np.array(training_accs),color='cornflowerblue',label=\"train\")\n",
    "ax[1].plot(np.arange(1,len(validation_accs)+1),np.array(validation_accs),color='orange',label=\"validation\")\n",
    "ax[1].scatter(len(validation_losses),test_acc,color='green',label=\"test\",marker=\"x\")\n",
    "ax[1].set_title(\"accuracy\"); ax[1].legend()\n",
    "\n",
    "ax[2].plot(np.arange(1,len(training_f1ms)+1),np.array(training_f1ms),color='cornflowerblue',label=\"train\")\n",
    "ax[2].plot(np.arange(1,len(validation_f1ms)+1),np.array(validation_f1ms),color='orange',label=\"validation\")\n",
    "ax[2].scatter(len(validation_losses),test_f1m,color='green',label=\"test\",marker=\"x\")\n",
    "ax[2].set_title(\"macro f1\"); ax[2].legend()\n",
    "\n",
    "for lrd in lr_decreases:\n",
    "    ax[0].axvline(x=lrd, ymin=0, ymax=1, color='grey')\n",
    "    ax[1].axvline(x=lrd, ymin=0, ymax=1, color='grey')\n",
    "    ax[2].axvline(x=lrd, ymin=0, ymax=1, color='grey')\n",
    "\n",
    "ax[0].axvline(x=best_val_epoch, ymin=0, ymax=1, color='red',alpha=0.3)\n",
    "ax[1].axvline(x=best_val_epoch, ymin=0, ymax=1, color='red',alpha=0.3)\n",
    "ax[2].axvline(x=best_val_epoch, ymin=0, ymax=1, color='red',alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "if not is_local:\n",
    "    plt.savefig(f'./loss_curve_{seed}.png')\n",
    "    plt.close(fig)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-04-09T13:58:56.168693600Z"
    }
   },
   "id": "5aa6f4abb81d43a8",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(2,3,figsize=(15,5)); ax = ax.flatten()\n",
    "for cls in range(6):\n",
    "    ax[cls].plot(np.arange(1,len(training_f1s)+1),[i[cls] for i in training_f1s], color='black', label=\"train\")\n",
    "    ax[cls].plot(np.arange(1,len(validation_f1s)+1),[i[cls] for i in validation_f1s], color=annotation_class_colors[cls]/255, label=\"val\")\n",
    "    ax[cls].set_title(f\"{annotation_class_names[cls]}\")\n",
    "    ax[cls].legend()\n",
    "    ax[cls].scatter(len(validation_losses),test_f1[cls],color='green',label=\"test\",marker=\"x\")\n",
    "    for lrd in lr_decreases:\n",
    "        ax[cls].axvline(x=lrd, ymin=0, ymax=1, color='grey')\n",
    "    ax[cls].axvline(x=best_val_epoch, ymin=0, ymax=1, color='red',alpha=0.3)\n",
    "fig.suptitle(\"Class-specific F1 scores\")\n",
    "plt.tight_layout()\n",
    "if not is_local:\n",
    "    plt.savefig(f'./loss_curve_by_class_{seed}.png')\n",
    "    plt.close(fig)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-09T13:58:56.173788300Z",
     "start_time": "2025-04-09T13:58:56.169690600Z"
    }
   },
   "id": "60c4c1d2914ee636",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Finish experiment"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5cd7e4042615b52"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "if not is_local:\n",
    "    model = model.cpu()\n",
    "    torch.save(model.state_dict(), rf'./model_weights_{seed}.pt')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-04-09T13:58:56.170763200Z"
    }
   },
   "id": "b8248f10250d5d36",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Read existing results file\n",
    "if not is_local:\n",
    "    if os.path.isfile('results.txt'):\n",
    "        f = open('results.txt','r')\n",
    "        lines = f.readlines()\n",
    "        f.close()\n",
    "    else: \n",
    "        lines = [x+', \\n' for x in['seed',*annotation_class_names,'overall_acc','macro_f1']]\n",
    "        \n",
    "    # Process files\n",
    "    lines[0] = lines[0].replace('\\n',str(seed) + ', \\n')\n",
    "    for cls in range(n_classes):\n",
    "        lines[cls+1] = lines[cls+1].replace('\\n',str(test_f1[cls]) + ', \\n' )\n",
    "    lines[n_classes+1] = lines[n_classes+1].replace('\\n',str(test_acc) + ', \\n')\n",
    "    lines[n_classes+2] = lines[n_classes+2].replace('\\n',str(test_f1m) + ', \\n')\n",
    "    \n",
    "    f = open('results.txt','w')\n",
    "    f.write(''.join(lines))\n",
    "    f.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-04-09T13:58:56.171760200Z"
    }
   },
   "id": "bcb3c45e041764e8",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(12,12))\n",
    "locarray = np.zeros((model.encoder.pos_embedding.shape[1]-1,model.encoder.pos_embedding.shape[1]-1))\n",
    "cos = nn.CosineSimilarity(dim=0)\n",
    "for r in range(model.encoder.pos_embedding.shape[1]-1):\n",
    "    for c in range(model.encoder.pos_embedding.shape[1]-1):\n",
    "        locarray[r,c] = cos(model.encoder.pos_embedding[0][r + 1],model.encoder.pos_embedding[0][c + 1]).detach().cpu().numpy()\n",
    "ax.set_title(\"position embeddings\")\n",
    "ax.matshow(locarray)\n",
    "fig.tight_layout()\n",
    "if not is_local:\n",
    "    plt.savefig(f'./position_embedding_{seed}.png'); plt.close(fig) "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-04-09T13:58:56.172791400Z"
    }
   },
   "id": "f9edf036d6287a1f",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
