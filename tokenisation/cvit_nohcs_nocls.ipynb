{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-08T10:23:27.472916800Z",
     "start_time": "2025-04-08T10:23:10.601684800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import openpyxl\n",
    "import h5py\n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.transforms import v2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.decomposition import PCA\n",
    "import sys\n",
    "import os\n",
    "import random \n",
    "import warnings\n",
    "import math\n",
    "from functools import partial\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Experiment Hyperparameters"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5c3fa5c69dc9bda7"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "is_local = True # todo\n",
    "\n",
    "# Experiment\n",
    "seed = 1000 if is_local else int(sys.argv[-2])\n",
    "torch.manual_seed(seed)\n",
    "image_size = 256\n",
    "\n",
    "# Data: which wavenumbers are even allowed to be considered?\n",
    "wv_start = 0\n",
    "wv_end = 965\n",
    "\n",
    "# Data loading\n",
    "test_set_fraction = 0.2\n",
    "val_set_fraction = 0.2\n",
    "batch_size= 2 # todo see how high can get on csf\n",
    "use_augmentation = True\n",
    "\n",
    "# Network\n",
    "dropout_p=0\n",
    "\n",
    "# Training schedule\n",
    "lr = 1e-5\n",
    "l2 = 5e-2\n",
    "max_epochs=200\n",
    "\n",
    "# dimensionality reduction parameters\n",
    "r_method = 'linear' # {'linear','pca,'fixed'}\n",
    "reduce_dim = 16 if is_local else int(sys.argv[-1]) # todo \n",
    "channels_used = np.s_[...,wv_start:wv_end] # used only when r_method = 'fixed'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-08T10:23:27.551009700Z",
     "start_time": "2025-04-08T10:23:27.474938800Z"
    }
   },
   "id": "222959fcfa561aff",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 228 cores\n",
      "Using 965/965 wavenumbers\n"
     ]
    }
   ],
   "source": [
    "def csf_fp(filepath):\n",
    "    return filepath.replace('D:/datasets','D:/datasets' if is_local else './')\n",
    "\n",
    "master = pd.read_excel(csf_fp(rf'D:/datasets/pcuk2023_ftir_whole_core/master_sheet.xlsx'))\n",
    "slide = master['slide'].to_numpy()\n",
    "patient_id = master['patient_id'].to_numpy()\n",
    "hdf5_filepaths = np.array([csf_fp(fp) for fp in master['hdf5_filepath']])\n",
    "annotation_filepaths = np.array([csf_fp(fp) for fp in master['annotation_filepath']])\n",
    "mask_filepaths = np.array([csf_fp(fp) for fp in master['mask_filepath']])\n",
    "wavenumbers = np.load(csf_fp(f'D:/datasets/pcuk2023_ftir_whole_core/wavenumbers.npy'))[wv_start:wv_end]\n",
    "wavenumbers_used = wavenumbers[channels_used]\n",
    "\n",
    "annotation_class_colors = np.array([[0,255,0],[128,0,128],[255,0,255],[0,0,255],[255,165,0],[255,0,0]])\n",
    "annotation_class_names = np.array(['epithelium_n','stroma_n','epithelium_c','stroma_c','corpora_amylacea','blood'])\n",
    "n_classes = len(annotation_class_names)\n",
    "print(f\"Loaded {len(slide)} cores\")\n",
    "print(f\"Using {len(wavenumbers_used)}/{len(wavenumbers)} wavenumbers\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-08T10:23:27.598621800Z",
     "start_time": "2025-04-08T10:23:27.552006600Z"
    }
   },
   "id": "a78af96389a4cd31",
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define Datasets, Dataloaders"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aaba53c2e93ca461"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patients per data split:\n",
      "\tTRAIN: 130\n",
      "\tVAL: 51\n",
      "\tTEST: 47\n"
     ]
    }
   ],
   "source": [
    "unique_pids = np.unique(patient_id)\n",
    "pids_trainval, pids_test, _, _ = train_test_split(\n",
    "    unique_pids, np.zeros_like(unique_pids), test_size=test_set_fraction, random_state=seed)\n",
    "pids_train, pids_val, _, _ = train_test_split(\n",
    "    pids_trainval, np.zeros_like(pids_trainval), test_size=(val_set_fraction/(1-test_set_fraction)), random_state=seed)\n",
    "where_train = np.where(np.isin(patient_id,pids_train))\n",
    "where_val = np.where(np.isin(patient_id,pids_val))\n",
    "where_test = np.where(np.isin(patient_id,pids_test))\n",
    "print(f\"Patients per data split:\\n\\tTRAIN: {len(where_train[0])}\\n\\tVAL: {len(where_val[0])}\\n\\tTEST: {len(where_test[0])}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-08T10:23:27.604200200Z",
     "start_time": "2025-04-08T10:23:27.601642500Z"
    }
   },
   "id": "e4655cf38851b265",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class ftir_annot_dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, \n",
    "                 hdf5_filepaths, mask_filepaths, annotation_filepaths, channels_use, augment=False):\n",
    "        self.hdf5_filepaths = hdf5_filepaths\n",
    "        self.mask_filepaths = mask_filepaths\n",
    "        self.annotation_filepaths = annotation_filepaths\n",
    "        self.channels_use = channels_use\n",
    "        self.augment=augment\n",
    "        \n",
    "        # class data\n",
    "        self.annotation_class_colors = np.array([[0,255,0],[128,0,128],[255,0,255],[0,0,255],[255,165,0],[255,0,0]])\n",
    "        self.annotation_class_names = np.array(['epithelium_n','stroma_n','epithelium_c','stroma_c','corpora_amylacea','blood'])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.hdf5_filepaths)\n",
    "    \n",
    "    # split annotations from H x W x 3 to C x H x W, one/zerohot along C dimension\n",
    "    def split_annotations(self,annotations_img):\n",
    "        split = torch.zeros((len(self.annotation_class_colors),*annotations_img.shape[:-1]))\n",
    "        for c,col in enumerate(annotation_class_colors):\n",
    "            split[c,:,:] = torch.from_numpy(np.all(annotations_img == self.annotation_class_colors[c],axis=-1)) \n",
    "        return split\n",
    "        \n",
    "    def __getitem__(self, idx):    \n",
    "        \n",
    "        # open hdf5 file\n",
    "        hdf5_file = h5py.File(self.hdf5_filepaths[idx],'r')\n",
    "        \n",
    "        # get mask\n",
    "        mask = torch.from_numpy(\n",
    "            hdf5_file['mask'][:],\n",
    "        ).unsqueeze(0)\n",
    "        \n",
    "        # get ftir\n",
    "        ftir = torch.from_numpy(\n",
    "            hdf5_file['spectra'][*self.channels_use],\n",
    "        ).permute(2,0,1)\n",
    "        hdf5_file.close()\n",
    "        ftir *= mask\n",
    "        \n",
    "        # get annotations\n",
    "        annotations = self.split_annotations(cv2.imread(self.annotation_filepaths[idx])[:,:,::-1])\n",
    "        annotations *= mask\n",
    "        has_annotations = annotations.sum(dim=0) != 0\n",
    "        \n",
    "        if self.augment:\n",
    "            to_aug = torch.rand((2,))\n",
    "            if to_aug[0] > 0.5: #hflip\n",
    "                ftir = torch.flip(ftir, (-1,))\n",
    "                annotations = torch.flip(annotations, (-1,))\n",
    "                has_annotations = torch.flip(has_annotations, (-1,))\n",
    "                mask = torch.flip(mask, (-1,))\n",
    "            if to_aug[1] > 0.5: #vflip\n",
    "                ftir = torch.flip(ftir, (-2,))\n",
    "                annotations = torch.flip(annotations, (-2,))\n",
    "                has_annotations = torch.flip(has_annotations, (-2,))\n",
    "                mask = torch.flip(mask, (-2,))\n",
    "        \n",
    "        return ftir, annotations, mask, has_annotations"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-08T10:23:27.611094500Z",
     "start_time": "2025-04-08T10:23:27.605197100Z"
    }
   },
   "id": "a8a3aa59fbf57012",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loader sizes:\n",
      "\ttrain: 65\n",
      "\tval: 25\n",
      "\ttest: 23\n"
     ]
    }
   ],
   "source": [
    "dataset_train = ftir_annot_dataset(\n",
    "    hdf5_filepaths[where_train], mask_filepaths[where_train], annotation_filepaths[where_train], channels_used, augment=use_augmentation,\n",
    ")\n",
    "dataset_val = ftir_annot_dataset(\n",
    "    hdf5_filepaths[where_val], mask_filepaths[where_val], annotation_filepaths[where_val], channels_used, augment=False,\n",
    ")\n",
    "dataset_test = ftir_annot_dataset(\n",
    "    hdf5_filepaths[where_test], mask_filepaths[where_test], annotation_filepaths[where_test], channels_used, augment=False,\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size, shuffle=True,drop_last=True)\n",
    "val_loader = torch.utils.data.DataLoader(dataset_val, batch_size=batch_size, shuffle=True,drop_last=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset_test, batch_size=batch_size,shuffle=True,drop_last=True)\n",
    "print(f\"loader sizes:\\n\\ttrain: {len(train_loader)}\\n\\tval: {len(val_loader)}\\n\\ttest: {len(test_loader)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-08T10:23:27.615395600Z",
     "start_time": "2025-04-08T10:23:27.611094500Z"
    }
   },
   "id": "c6bebd9eeed34711",
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define dimensionality reduction method"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "27b5434becc5286e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class LinearReduction(nn.Module):\n",
    "    def __init__(self,input_dim,reduce_dim):\n",
    "        super().__init__()\n",
    "        self.reduce_dim = reduce_dim\n",
    "        self.input_norm = nn.BatchNorm2d(input_dim)\n",
    "        self.projection = nn.Conv2d(input_dim,reduce_dim,kernel_size=1,stride=1)\n",
    "        self.projection_norm = nn.BatchNorm2d(reduce_dim)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self.projection_norm(self.projection(self.input_norm(x)))\n",
    "    \n",
    "class PCAReduce(nn.Module):\n",
    "    def __init__(self,reduce_dim,means,loadings):\n",
    "        super().__init__()\n",
    "        self.reduce_dim = reduce_dim\n",
    "        self.register_buffer('means', torch.from_numpy(means).float().reshape(1,-1,1,1))\n",
    "        self.register_buffer('loadings', torch.from_numpy(loadings).float())\n",
    "    \n",
    "    def forward(self,x):\n",
    "        projected = x - self.means\n",
    "        \n",
    "        b,c,h,w = projected.shape\n",
    "        projected = projected.permute(0,2,3,1).reshape(b,h*w,c)\n",
    "        projected = torch.matmul(projected, self.loadings.T)\n",
    "        projected = projected.reshape(b,h,w,self.reduce_dim).permute(0,3,1,2)\n",
    "        \n",
    "        return projected\n",
    "        \n",
    "class FixedReduction(nn.Module):\n",
    "    def __init__(self,input_dim):\n",
    "        super().__init__()\n",
    "        self.input_norm = nn.BatchNorm2d(input_dim)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self.input_norm(x)\n",
    "\n",
    "if r_method == 'pca':\n",
    "    spectral_sample = []\n",
    "    batch_samples = 0\n",
    "    for data, annotations, mask, has_annotations in train_loader:\n",
    "        where = torch.where(has_annotations[0] == 1)\n",
    "        ridxs = torch.randperm(where[0].shape[0])[:100]\n",
    "        spectral_sample.append(data[:, :, where[0][ridxs],where[1][ridxs]].permute(0,2,1).flatten(0,1).numpy())\n",
    "        batch_samples += 1\n",
    "        if batch_samples > 10: break\n",
    "    spectral_sample = np.concatenate(spectral_sample,axis=0)\n",
    "    spectral_means = np.mean(spectral_sample,axis=0)\n",
    "    spectral_sample -= spectral_means\n",
    "    pca = PCA(n_components=reduce_dim)\n",
    "    pca.fit(spectral_sample)\n",
    "    spectral_loadings = pca.components_\n",
    "    \n",
    "if r_method == 'pca':\n",
    "    input_processing = PCAReduce(reduce_dim,spectral_means,spectral_loadings)\n",
    "elif r_method == 'fixed':\n",
    "    input_processing = FixedReduction(input_dim=len(wavenumbers_used))\n",
    "elif r_method == 'linear':\n",
    "    input_processing = LinearReduction(input_dim=len(wavenumbers_used),reduce_dim=reduce_dim)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-08T10:23:27.640880200Z",
     "start_time": "2025-04-08T10:23:27.618483900Z"
    }
   },
   "id": "b5f1cfe137e09f85",
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define Model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f550708e9f4c74c5"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def _no_grad_trunc_normal_(tensor, mean, std, a, b):\n",
    "    # Cut & paste from PyTorch official master until it's in a few official releases - RW\n",
    "    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf\n",
    "    def norm_cdf(x):\n",
    "        # Computes standard normal cumulative distribution function\n",
    "        return (1.0 + math.erf(x / math.sqrt(2.0))) / 2.0\n",
    "\n",
    "    if (mean < a - 2 * std) or (mean > b + 2 * std):\n",
    "        warnings.warn(\n",
    "            \"mean is more than 2 std from [a, b] in nn.init.trunc_normal_. \"\n",
    "            \"The distribution of values may be incorrect.\",\n",
    "            stacklevel=2,\n",
    "        )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Values are generated by using a truncated uniform distribution and\n",
    "        # then using the inverse CDF for the normal distribution.\n",
    "        # Get upper and lower cdf values\n",
    "        l = norm_cdf((a - mean) / std)\n",
    "        u = norm_cdf((b - mean) / std)\n",
    "\n",
    "        # Uniformly fill tensor with values from [l, u], then translate to\n",
    "        # [2l-1, 2u-1].\n",
    "        tensor.uniform_(2 * l - 1, 2 * u - 1)\n",
    "\n",
    "        # Use inverse cdf transform for normal distribution to get truncated\n",
    "        # standard normal\n",
    "        tensor.erfinv_()\n",
    "\n",
    "        # Transform to proper mean, std\n",
    "        tensor.mul_(std * math.sqrt(2.0))\n",
    "        tensor.add_(mean)\n",
    "\n",
    "        # Clamp to ensure it's in the proper range\n",
    "        tensor.clamp_(min=a, max=b)\n",
    "        return tensor\n",
    "\n",
    "def trunc_normal_(tensor, mean=0.0, std=1.0, a=-2.0, b=2.0):\n",
    "    # type: (Tensor, float, float, float, float) -> Tensor\n",
    "    return _no_grad_trunc_normal_(tensor, mean, std, a, b)\n",
    "\n",
    "class PatchEmbedPerChannel(nn.Module):\n",
    "    \"\"\"Image to Patch Embedding.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_size: int = 224,\n",
    "        patch_size: int = 16,\n",
    "        in_chans: int = 3,\n",
    "        embed_dim: int = 768,\n",
    "        enable_sample: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        num_patches = (img_size // patch_size) * (img_size // patch_size) * in_chans\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "\n",
    "        self.proj = nn.Conv3d(\n",
    "            1,\n",
    "            embed_dim,\n",
    "            kernel_size=(1, patch_size, patch_size),\n",
    "            stride=(1, patch_size, patch_size),\n",
    "        )  # CHANGED\n",
    "\n",
    "        self.channel_embed = nn.Embedding(in_chans, embed_dim)\n",
    "        self.enable_sample = enable_sample\n",
    "\n",
    "        trunc_normal_(self.channel_embed.weight, std=0.02)\n",
    "\n",
    "    def forward(self, x, extra_tokens={}):\n",
    "        # # assume all images in the same batch has the same input channels\n",
    "        # cur_channels = extra_tokens[\"channels\"][0]\n",
    "        # embedding lookup\n",
    "        cur_channel_embed = self.channel_embed(\n",
    "            extra_tokens[\"channels\"]\n",
    "        )  # B, Cin, embed_dim=Cout\n",
    "        cur_channel_embed = cur_channel_embed.permute(0, 2, 1)  # B Cout Cin\n",
    "\n",
    "        B, Cin, H, W = x.shape\n",
    "        # Note: The current number of channels (Cin) can be smaller or equal to in_chans\n",
    "\n",
    "        # shared projection layer across channels\n",
    "        x = self.proj(x.unsqueeze(1))  # B Cout Cin H W\n",
    "\n",
    "        # channel specific offsets\n",
    "        x += cur_channel_embed.unsqueeze(-1).unsqueeze(-1)\n",
    "        # x += self.channel_embed[:, :, cur_channels, :, :]  # B Cout Cin H W\n",
    "\n",
    "        # preparing the output sequence\n",
    "        x = x.flatten(2)  # B Cout CinHW\n",
    "        x = x.transpose(1, 2)  # B CinHW Cout\n",
    "\n",
    "        return x, Cin\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        num_heads=8,\n",
    "        qkv_bias=False,\n",
    "        qk_scale=None,\n",
    "        attn_drop=0.0,\n",
    "        proj_drop=0.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = (\n",
    "            self.qkv(x)\n",
    "            .reshape(B, N, 3, self.num_heads, C // self.num_heads)\n",
    "            .permute(2, 0, 3, 1, 4)\n",
    "        )\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x, attn\n",
    "    \n",
    "class Mlp(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features,\n",
    "        hidden_features=None,\n",
    "        out_features=None,\n",
    "        act_layer=nn.GELU,\n",
    "        drop=0.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x    \n",
    "\n",
    "def drop_path(x, drop_prob: float = 0.0, training: bool = False):\n",
    "    if drop_prob == 0.0 or not training:\n",
    "        return x\n",
    "    keep_prob = 1 - drop_prob\n",
    "    shape = (x.shape[0],) + (1,) * (\n",
    "        x.ndim - 1\n",
    "    )  # work with diff dim tensors, not just 2D ConvNets\n",
    "    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
    "    random_tensor.floor_()  # binarize\n",
    "    output = x.div(keep_prob) * random_tensor\n",
    "    return output\n",
    "\n",
    "class DropPath(nn.Module):\n",
    "    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\"\"\"\n",
    "\n",
    "    def __init__(self, drop_prob=None):\n",
    "        super(DropPath, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        return drop_path(x, self.drop_prob, self.training)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        num_heads,\n",
    "        mlp_ratio=4.0,\n",
    "        qkv_bias=False,\n",
    "        qk_scale=None,\n",
    "        drop=0.0,\n",
    "        attn_drop=0.0,\n",
    "        drop_path=0.0,\n",
    "        act_layer=nn.GELU,\n",
    "        norm_layer=nn.LayerNorm,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = Attention(\n",
    "            dim,\n",
    "            num_heads=num_heads,\n",
    "            qkv_bias=qkv_bias,\n",
    "            qk_scale=qk_scale,\n",
    "            attn_drop=attn_drop,\n",
    "            proj_drop=drop,\n",
    "        )\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(\n",
    "            in_features=dim,\n",
    "            hidden_features=mlp_hidden_dim,\n",
    "            act_layer=act_layer,\n",
    "            drop=drop,\n",
    "        )\n",
    "\n",
    "    def forward(self, x, return_attention=False):\n",
    "        y, attn = self.attn(self.norm1(x))\n",
    "        if return_attention:\n",
    "            return attn\n",
    "        x = x + self.drop_path(y)\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        return x\n",
    "\n",
    "class ChannelVisionTransformer(nn.Module):\n",
    "    \"\"\"Channel Vision Transformer\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_size=[224],\n",
    "        patch_size=16,\n",
    "        in_chans=3,\n",
    "        num_classes=0,\n",
    "        embed_dim=768,\n",
    "        depth=12,\n",
    "        num_heads=12,\n",
    "        mlp_ratio=4.0,\n",
    "        qkv_bias=False,\n",
    "        qk_scale=None,\n",
    "        drop_rate=0.0,\n",
    "        attn_drop_rate=0.0,\n",
    "        drop_path_rate=0.0,\n",
    "        norm_layer=nn.LayerNorm,\n",
    "        enable_sample=True,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        print(\n",
    "            \"Warning!!!\\n\"\n",
    "            \"Samplev2 channel vit randomly sample channels for each batch.\\n\"\n",
    "            \"It is only compatible with Supervised learning\\n\"\n",
    "            \"Doesn't work with DINO or Linear Prob\"\n",
    "        )\n",
    "\n",
    "        self.num_features = self.embed_dim = self.out_dim = embed_dim\n",
    "        self.in_chans = in_chans\n",
    "        \n",
    "        if r_method == 'pca':\n",
    "            self.input_processing = PCAReduce(reduce_dim,spectral_means,spectral_loadings)\n",
    "        elif r_method == 'fixed':\n",
    "            self.input_processing = FixedReduction(input_dim=len(wavenumbers_used))\n",
    "        elif r_method == 'linear':\n",
    "            self.input_processing = LinearReduction(input_dim=len(wavenumbers_used),reduce_dim=reduce_dim)\n",
    "\n",
    "        self.patch_embed = PatchEmbedPerChannel(\n",
    "            img_size=img_size[0],\n",
    "            patch_size=patch_size,\n",
    "            in_chans=in_chans,\n",
    "            embed_dim=embed_dim,\n",
    "            enable_sample=enable_sample,\n",
    "        )\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "        self.patches_per_channel = self.patch_embed.num_patches // reduce_dim\n",
    "        self.patch_size=patch_size\n",
    "\n",
    "        self.num_extra_tokens = 0  # cls token\n",
    "\n",
    "        self.pos_embed = nn.Parameter(\n",
    "            torch.zeros(\n",
    "                1, num_patches // self.in_chans + self.num_extra_tokens, embed_dim\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        dpr = [\n",
    "            x.item() for x in torch.linspace(0, drop_path_rate, depth)\n",
    "        ]  # stochastic depth decay rule\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [\n",
    "                Block(\n",
    "                    dim=embed_dim,\n",
    "                    num_heads=num_heads,\n",
    "                    mlp_ratio=mlp_ratio,\n",
    "                    qkv_bias=qkv_bias,\n",
    "                    qk_scale=qk_scale,\n",
    "                    drop=drop_rate,\n",
    "                    attn_drop=attn_drop_rate,\n",
    "                    drop_path=dpr[i],\n",
    "                    norm_layer=norm_layer,\n",
    "                )\n",
    "                for i in range(depth)\n",
    "            ]\n",
    "        )\n",
    "        self.norm = norm_layer(embed_dim)\n",
    "\n",
    "        # Classifier head\n",
    "        self.upscale = nn.ConvTranspose2d(embed_dim,embed_dim,kernel_size=16,stride=16)\n",
    "        self.out_conv = nn.Sequential(\n",
    "            nn.Conv2d(embed_dim+reduce_dim,embed_dim,kernel_size=3,stride=1,padding=1,bias=False),\n",
    "            nn.BatchNorm2d(embed_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(embed_dim,embed_dim,kernel_size=3,stride=1,padding=1,bias=False),\n",
    "            nn.BatchNorm2d(embed_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(embed_dim,n_classes,kernel_size=3,stride=1,padding=1),\n",
    "        )\n",
    "\n",
    "        trunc_normal_(self.pos_embed, std=0.02)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=0.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    def interpolate_pos_encoding(self, x, w, h, c):\n",
    "        num_extra_tokens = 0\n",
    "\n",
    "        npatch = x.shape[1] - num_extra_tokens\n",
    "        N = self.pos_embed.shape[1] - num_extra_tokens\n",
    "\n",
    "        if npatch == N and w == h:\n",
    "            return self.pos_embed\n",
    "\n",
    "        class_pos_embed = self.pos_embed[:, :num_extra_tokens]\n",
    "        patch_pos_embed = self.pos_embed[:, num_extra_tokens:]\n",
    "\n",
    "        dim = x.shape[-1]\n",
    "        w0 = w // self.patch_embed.patch_size\n",
    "        h0 = h // self.patch_embed.patch_size\n",
    "        # we add a small number to avoid floating point error in the interpolation\n",
    "        # see discussion at https://github.com/facebookresearch/dino/issues/8\n",
    "        w0, h0 = w0 + 0.1, h0 + 0.1\n",
    "        patch_pos_embed = nn.functional.interpolate(\n",
    "            patch_pos_embed.reshape(\n",
    "                1, int(math.sqrt(N)), int(math.sqrt(N)), dim\n",
    "            ).permute(0, 3, 1, 2),\n",
    "            scale_factor=(w0 / math.sqrt(N), h0 / math.sqrt(N)),\n",
    "            mode=\"bicubic\",\n",
    "        )\n",
    "        assert (\n",
    "            int(w0) == patch_pos_embed.shape[-2]\n",
    "            and int(h0) == patch_pos_embed.shape[-1]\n",
    "        )\n",
    "        patch_pos_embed = patch_pos_embed.permute(0, 2, 3, 1).view(1, 1, -1, dim)\n",
    "\n",
    "        # create copies of the positional embeddings for each channel\n",
    "        patch_pos_embed = patch_pos_embed.expand(1, c, -1, dim).reshape(1, -1, dim)\n",
    "\n",
    "        return torch.cat((class_pos_embed, patch_pos_embed), dim=1)\n",
    "\n",
    "    def prepare_tokens(self, x, extra_tokens):\n",
    "        B, nc, w, h = x.shape\n",
    "        x, nc = self.patch_embed(x, extra_tokens)  # patch linear embedding\n",
    "\n",
    "\n",
    "        # add positional encoding to each token\n",
    "        x = x + self.interpolate_pos_encoding(x, w, h, nc)\n",
    "\n",
    "        return self.pos_drop(x)\n",
    "\n",
    "    def forward(self, x, extra_tokens={}):\n",
    "        b,c,h,w = x.shape\n",
    "        x_proc = self.input_processing(x)\n",
    "        x = self.prepare_tokens(x_proc, extra_tokens)\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        x = self.norm(x)\n",
    "        # B x (hw x c) x d_model -> B x d_model x (hw x c) -> B x d_model x h x w x c -> B x d_model x h x w\n",
    "        x = x.permute(0,2,1).reshape(x.size(0),self.embed_dim,h//self.patch_size,w//self.patch_size,-1).mean(dim=-1)\n",
    "        # B x d_model x h x w -> B x 6 x H x W\n",
    "        x = self.upscale(x)\n",
    "        x = self.out_conv(torch.cat([x,x_proc],dim=1))\n",
    "        return x\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-08T10:24:52.644768900Z",
     "start_time": "2025-04-08T10:24:52.611797400Z"
    }
   },
   "id": "33535f38f1458959",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning!!!\n",
      "Samplev2 channel vit randomly sample channels for each batch.\n",
      "It is only compatible with Supervised learning\n",
      "Doesn't work with DINO or Linear Prob\n"
     ]
    }
   ],
   "source": [
    "model = ChannelVisionTransformer(\n",
    "    img_size=[image_size],\n",
    "    patch_size=16,\n",
    "    in_chans=reduce_dim, # todo change if not using linear reduc\n",
    "    num_classes=n_classes,\n",
    "    embed_dim=192, # todo\n",
    "    depth=12,\n",
    "    num_heads=12,\n",
    "    mlp_ratio=4,\n",
    "    qkv_bias=True,\n",
    "    norm_layer=partial(nn.LayerNorm, eps=1e-6),\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "chn_tok = {\"channels\":torch.from_numpy(np.arange(0,reduce_dim)).unsqueeze(0).to(device)}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-08T10:24:52.902745600Z",
     "start_time": "2025-04-08T10:24:52.737444900Z"
    }
   },
   "id": "1956e51b755a02cd",
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training Loop"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4dfdaf2ad556fa46"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss(reduction='none')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr,weight_decay=l2)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=15, threshold=0.01, cooldown=0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-08T10:24:53.058090500Z",
     "start_time": "2025-04-08T10:24:53.051775100Z"
    }
   },
   "id": "50a59f2ddf1e3f0d",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "training_losses,validation_losses = [],[]\n",
    "training_accs,validation_accs = [],[]\n",
    "training_f1ms,validation_f1ms = [],[]\n",
    "training_f1s,validation_f1s = [],[]\n",
    "lr_decreases = []\n",
    "current_iters = 0\n",
    "best_val_f1 = 0\n",
    "best_val_iter = 0\n",
    "stop_training=False"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-08T10:24:53.148501900Z",
     "start_time": "2025-04-08T10:24:53.144024100Z"
    }
   },
   "id": "51f82808428ba870",
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ✰ ✰ ✰ EPOCH 1 ✰ ✰ ✰ \n",
      "train : █\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[17], line 20\u001B[0m\n\u001B[0;32m     18\u001B[0m \u001B[38;5;66;03m# Push data through model\u001B[39;00m\n\u001B[0;32m     19\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m---> 20\u001B[0m out \u001B[38;5;241m=\u001B[39m model(data,extra_tokens\u001B[38;5;241m=\u001B[39mchn_tok)\n\u001B[0;32m     22\u001B[0m \u001B[38;5;66;03m# Calculate loss\u001B[39;00m\n\u001B[0;32m     23\u001B[0m loss \u001B[38;5;241m=\u001B[39m loss_fn(out,annot\u001B[38;5;241m.\u001B[39margmax(dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)) \u001B[38;5;241m*\u001B[39m has_annotations \u001B[38;5;66;03m# loss per pixel\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\anaconda3\\envs\\phd-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\AppData\\Local\\anaconda3\\envs\\phd-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "Cell \u001B[1;32mIn[13], line 380\u001B[0m, in \u001B[0;36mChannelVisionTransformer.forward\u001B[1;34m(self, x, extra_tokens)\u001B[0m\n\u001B[0;32m    378\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprepare_tokens(x_proc, extra_tokens)\n\u001B[0;32m    379\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m blk \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mblocks:\n\u001B[1;32m--> 380\u001B[0m     x \u001B[38;5;241m=\u001B[39m blk(x)\n\u001B[0;32m    381\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnorm(x)\n\u001B[0;32m    382\u001B[0m \u001B[38;5;66;03m# B x (hw x c) x d_model -> B x d_model x (hw x c) -> B x d_model x h x w x c -> B x d_model x h x w\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\anaconda3\\envs\\phd-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\AppData\\Local\\anaconda3\\envs\\phd-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "Cell \u001B[1;32mIn[13], line 216\u001B[0m, in \u001B[0;36mBlock.forward\u001B[1;34m(self, x, return_attention)\u001B[0m\n\u001B[0;32m    215\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x, return_attention\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[1;32m--> 216\u001B[0m     y, attn \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mattn(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnorm1(x))\n\u001B[0;32m    217\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m return_attention:\n\u001B[0;32m    218\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m attn\n",
      "File \u001B[1;32m~\\AppData\\Local\\anaconda3\\envs\\phd-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\AppData\\Local\\anaconda3\\envs\\phd-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "Cell \u001B[1;32mIn[13], line 126\u001B[0m, in \u001B[0;36mAttention.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m    123\u001B[0m q, k, v \u001B[38;5;241m=\u001B[39m qkv[\u001B[38;5;241m0\u001B[39m], qkv[\u001B[38;5;241m1\u001B[39m], qkv[\u001B[38;5;241m2\u001B[39m]\n\u001B[0;32m    125\u001B[0m attn \u001B[38;5;241m=\u001B[39m (q \u001B[38;5;241m@\u001B[39m k\u001B[38;5;241m.\u001B[39mtranspose(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)) \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mscale\n\u001B[1;32m--> 126\u001B[0m attn \u001B[38;5;241m=\u001B[39m attn\u001B[38;5;241m.\u001B[39msoftmax(dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m    127\u001B[0m attn \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mattn_drop(attn)\n\u001B[0;32m    129\u001B[0m x \u001B[38;5;241m=\u001B[39m (attn \u001B[38;5;241m@\u001B[39m v)\u001B[38;5;241m.\u001B[39mtranspose(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m)\u001B[38;5;241m.\u001B[39mreshape(B, N, C)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(max_epochs):\n",
    "    print(f\"\\n ✰ ✰ ✰ EPOCH {epoch+1} ✰ ✰ ✰ \")\n",
    "    \n",
    "    # reset running metrics\n",
    "    running_loss_train, running_loss_val = 0, 0\n",
    "    train_preds,train_targets = [],[]\n",
    "    val_preds,val_targets = [],[]\n",
    "    \n",
    "    # Train loop\n",
    "    model.train()\n",
    "    batch_frac = 42 / (len(train_loader))\n",
    "    for batch_idx, (data, annot, mask, has_annotations) in enumerate(train_loader):\n",
    "        print(f\"train : {'█'*int(batch_idx*batch_frac)}\", end=\"\\r\")\n",
    "        \n",
    "        # Put data and label on device\n",
    "        data = data.to(device); annot = annot.to(device); has_annotations = has_annotations.to(device)\n",
    "        \n",
    "        # Push data through model\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data,extra_tokens=chn_tok)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = loss_fn(out,annot.argmax(dim=1)) * has_annotations # loss per pixel\n",
    "        loss = loss.sum() / (has_annotations.sum()) # mean loss per annotated pixel\n",
    "        loss.backward() # backprop\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Calculate metrics\n",
    "        running_loss_train += loss.cpu().item()\n",
    "        targets = annot.argmax(dim=1)[has_annotations] # class targets on annotated pixels\n",
    "        preds = out.argmax(dim=1)[has_annotations] # predicted values on annotated pixels\n",
    "        train_preds.extend(preds.detach().cpu().numpy())\n",
    "        train_targets.extend(targets.detach().cpu().numpy())\n",
    "    print(f\"train : {'█'*42}\")\n",
    "        \n",
    "    # Validate loop\n",
    "    model.eval()\n",
    "    batch_frac = 42 / len(val_loader)\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, annot, mask, has_annotations) in enumerate(val_loader):\n",
    "            print(f\"val   : {'█'*int(batch_idx*batch_frac)}\", end=\"\\r\")\n",
    "            \n",
    "            # Put data and label on device\n",
    "            data = data.to(device); annot = annot.to(device); has_annotations = has_annotations.to(device)\n",
    "            \n",
    "            # Push data through model\n",
    "            out = model(data,extra_tokens=chn_tok)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = loss_fn(out,annot.argmax(dim=1)) * has_annotations # loss per pixel\n",
    "            loss = loss.sum() / (has_annotations.sum()) # mean loss per annotated pixel\n",
    "            \n",
    "            # Calculate metrics\n",
    "            running_loss_val += loss.cpu().item()\n",
    "            targets = annot.argmax(dim=1)[has_annotations] # class targets on annotated pixels\n",
    "            preds = out.argmax(dim=1)[has_annotations] # predicted values on annotated pixels\n",
    "            val_preds.extend(preds.detach().cpu().numpy())\n",
    "            val_targets.extend(targets.detach().cpu().numpy())\n",
    "    print(f\"val   : {'█'*42}\")\n",
    "    \n",
    "    # calculate epoch metrics for train set\n",
    "    train_acc = accuracy_score(train_targets, train_preds); training_accs.append(train_acc)\n",
    "    train_f1m = f1_score(train_targets, train_preds, average='macro'); training_f1ms.append(train_f1m)\n",
    "    train_f1 = f1_score(train_targets, train_preds, average=None); training_f1s.append(train_f1)\n",
    "    train_loss = running_loss_train / (len(dataset_train)); training_losses.append(train_loss)\n",
    "    \n",
    "    # calculate epoch metrics for val set\n",
    "    val_acc = accuracy_score(val_targets, val_preds); validation_accs.append(val_acc)\n",
    "    val_f1m = f1_score(val_targets, val_preds, average='macro'); validation_f1ms.append(val_f1m)\n",
    "    val_f1 = f1_score(val_targets, val_preds, average=None); validation_f1s.append(val_f1)\n",
    "    val_loss = running_loss_val / (len(dataset_val)); validation_losses.append(val_loss)\n",
    "    \n",
    "    # Update\n",
    "    print(f\"TRAIN --- | Loss: {train_loss:.4} | OA: {train_acc:.4} | f1: {train_f1m:.4}\")\n",
    "    print(f\"VAL ----- | Loss: {val_loss:.4} | OA: {val_acc:.4} | f1: {val_f1m:.4}\")\n",
    "    \n",
    "    scheduler.step(val_f1m)\n",
    "    new_lr = optimizer.param_groups[0]['lr']\n",
    "    if new_lr != lr:\n",
    "        print(f\"Val f1 plateaued, lr {lr} -> {new_lr}\")\n",
    "        lr = new_lr\n",
    "        lr_decreases.append(epoch)\n",
    "        if len(lr_decreases) >= 3: \n",
    "            print(\"Val f1 decreased thrice, ending training early\")\n",
    "            break\n",
    "\n",
    "    if val_f1m > best_val_f1:\n",
    "        best_val_f1 = val_f1m\n",
    "        best_val_epoch = epoch\n",
    "        if not is_local:\n",
    "            torch.save(model.state_dict(), rf'./model_weights_{seed}.pt')\n",
    "\n",
    "if not is_local:\n",
    "    model.load_state_dict(torch.load(rf'./model_weights_{seed}.pt', weights_only=True))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-08T10:26:56.867471800Z",
     "start_time": "2025-04-08T10:24:53.349672600Z"
    }
   },
   "id": "7c6adbd14fd799f6",
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test Loop"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "307cf693fb06cb7e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Test\n",
    "running_loss_test = 0\n",
    "test_preds, test_targets = [], []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (data, annot, mask, has_annotations) in enumerate(test_loader):\n",
    "        # Put data and label on device\n",
    "        data = data.to(device); annot = annot.to(device); has_annotations = has_annotations.to(device)\n",
    "        \n",
    "        # Push data through model\n",
    "        out = model(data,extra_tokens=chn_tok)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = loss_fn(out,annot.argmax(dim=1)) * has_annotations # loss per pixel\n",
    "        loss = loss.sum() / (has_annotations.sum()) # mean loss per annotated pixel\n",
    "        \n",
    "        # Calculate metrics\n",
    "        running_loss_test += loss.cpu().item()\n",
    "        targets = annot.argmax(dim=1)[has_annotations] # class targets on annotated pixels\n",
    "        preds = out.argmax(dim=1)[has_annotations] # predicted values on annotated pixels\n",
    "        test_preds.extend(preds.detach().cpu().numpy())\n",
    "        test_targets.extend(targets.detach().cpu().numpy())\n",
    "        \n",
    "        # Save pred figure todo remove\n",
    "        if is_local:\n",
    "            for b in range(data.shape[0]):\n",
    "                bidx = (batch_idx*1) + b\n",
    "                fig, ax = plt.subplots(figsize=(8, 4))\n",
    "                predcore = annotation_class_colors[out[b].argmax(dim=0).detach().cpu().numpy()].reshape(256,256,3) / 255\n",
    "                annotcolour = annotation_class_colors[annot[b].argmax(dim=0).cpu().numpy()] / 255\n",
    "                predcore *= mask[b,0].cpu().numpy()[...,np.newaxis]\n",
    "                annotcolour *= has_annotations[b].cpu().numpy()[...,np.newaxis]\n",
    "                annotcolour += mask[b,0].cpu().numpy()[...,np.newaxis] * 1 - has_annotations[b].cpu().numpy()[...,np.newaxis]\n",
    "                ax.imshow(np.hstack([predcore,annotcolour]))\n",
    "                ax.set_axis_off()\n",
    "                ax.text(235, 220, dataset_test.hdf5_filepaths[bidx].split('/')[-1][:-3], fontsize=12, color='cyan', fontweight='bold')\n",
    "                fig.tight_layout()\n",
    "\n",
    "# calculate test set metrics\n",
    "test_acc = accuracy_score(test_targets, test_preds)\n",
    "test_f1m = f1_score(test_targets, test_preds, average='macro')\n",
    "test_f1 = f1_score(test_targets, test_preds, average=None)\n",
    "test_loss = running_loss_test / batch_idx\n",
    "\n",
    "print(f\"TEST ---- | Loss: {test_loss:.4} | OA: {test_acc:.4} | f1: {test_f1m:.4}\")\n",
    "for cls_idx, f1 in enumerate(test_f1):\n",
    "    print(f\"{annotation_class_names[cls_idx]}{(20 - len(annotation_class_names[cls_idx])) * ' '} : {f1:.4}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-04-08T10:23:36.588328400Z"
    }
   },
   "id": "242a34c1a6fe3be8",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "67ff17a2ebecf4ae"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,3,figsize=(16,5))\n",
    "ax[0].plot(np.arange(1,len(training_losses)+1),np.array(training_losses),color='cornflowerblue',label=\"train\")\n",
    "ax[0].plot(np.arange(1,len(validation_losses)+1),np.array(validation_losses),color='orange',label=\"validation\")\n",
    "ax[0].scatter(len(validation_losses),test_loss,color='green',label=\"test\",marker=\"x\")\n",
    "ax[0].set_title(\"loss curves\"); ax[0].legend()\n",
    "\n",
    "ax[1].plot(np.arange(1,len(training_accs)+1),np.array(training_accs),color='cornflowerblue',label=\"train\")\n",
    "ax[1].plot(np.arange(1,len(validation_accs)+1),np.array(validation_accs),color='orange',label=\"validation\")\n",
    "ax[1].scatter(len(validation_losses),test_acc,color='green',label=\"test\",marker=\"x\")\n",
    "ax[1].set_title(\"accuracy\"); ax[1].legend()\n",
    "\n",
    "ax[2].plot(np.arange(1,len(training_f1ms)+1),np.array(training_f1ms),color='cornflowerblue',label=\"train\")\n",
    "ax[2].plot(np.arange(1,len(validation_f1ms)+1),np.array(validation_f1ms),color='orange',label=\"validation\")\n",
    "ax[2].scatter(len(validation_losses),test_f1m,color='green',label=\"test\",marker=\"x\")\n",
    "ax[2].set_title(\"macro f1\"); ax[2].legend()\n",
    "\n",
    "for lrd in lr_decreases:\n",
    "    ax[0].axvline(x=lrd, ymin=0, ymax=1, color='grey')\n",
    "    ax[1].axvline(x=lrd, ymin=0, ymax=1, color='grey')\n",
    "    ax[2].axvline(x=lrd, ymin=0, ymax=1, color='grey')\n",
    "\n",
    "ax[0].axvline(x=best_val_epoch, ymin=0, ymax=1, color='red',alpha=0.3)\n",
    "ax[1].axvline(x=best_val_epoch, ymin=0, ymax=1, color='red',alpha=0.3)\n",
    "ax[2].axvline(x=best_val_epoch, ymin=0, ymax=1, color='red',alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "if not is_local:\n",
    "    plt.savefig(f'./loss_curve_{seed}.png')\n",
    "    plt.close(fig)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-04-08T10:23:36.589382700Z"
    }
   },
   "id": "5aa6f4abb81d43a8",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(2,3,figsize=(15,5)); ax = ax.flatten()\n",
    "for cls in range(6):\n",
    "    ax[cls].plot(np.arange(1,len(training_f1s)+1),[i[cls] for i in training_f1s], color='black', label=\"train\")\n",
    "    ax[cls].plot(np.arange(1,len(validation_f1s)+1),[i[cls] for i in validation_f1s], color=annotation_class_colors[cls]/255, label=\"val\")\n",
    "    ax[cls].set_title(f\"{annotation_class_names[cls]}\")\n",
    "    ax[cls].legend()\n",
    "    ax[cls].scatter(len(validation_losses),test_f1[cls],color='green',label=\"test\",marker=\"x\")\n",
    "    for lrd in lr_decreases:\n",
    "        ax[cls].axvline(x=lrd, ymin=0, ymax=1, color='grey')\n",
    "    ax[cls].axvline(x=best_val_epoch, ymin=0, ymax=1, color='red',alpha=0.3)\n",
    "fig.suptitle(\"Class-specific F1 scores\")\n",
    "plt.tight_layout()\n",
    "if not is_local:\n",
    "    plt.savefig(f'./loss_curve_by_class_{seed}.png')\n",
    "    plt.close(fig)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-04-08T10:23:36.591413100Z"
    }
   },
   "id": "60c4c1d2914ee636",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Finish experiment"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5cd7e4042615b52"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "if not is_local:\n",
    "    model = model.cpu()\n",
    "    torch.save(model.state_dict(), rf'./model_weights_{seed}.pt')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-04-08T10:23:36.592410200Z"
    }
   },
   "id": "b8248f10250d5d36"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Read existing results file\n",
    "if not is_local:\n",
    "    if os.path.isfile('results.txt'):\n",
    "        f = open('results.txt','r')\n",
    "        lines = f.readlines()\n",
    "        f.close()\n",
    "    else: \n",
    "        lines = [x+', \\n' for x in['seed',*annotation_class_names,'overall_acc','macro_f1']]\n",
    "        \n",
    "    # Process files\n",
    "    lines[0] = lines[0].replace('\\n',str(seed) + ', \\n')\n",
    "    for cls in range(n_classes):\n",
    "        lines[cls+1] = lines[cls+1].replace('\\n',str(test_f1[cls]) + ', \\n' )\n",
    "    lines[n_classes+1] = lines[n_classes+1].replace('\\n',str(test_acc) + ', \\n')\n",
    "    lines[n_classes+2] = lines[n_classes+2].replace('\\n',str(test_f1m) + ', \\n')\n",
    "    \n",
    "    f = open('results.txt','w')\n",
    "    f.write(''.join(lines))\n",
    "    f.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-04-08T10:23:36.594443200Z"
    }
   },
   "id": "bcb3c45e041764e8",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,2,figsize=(12,6),dpi=200)\n",
    "locarray = np.zeros((model.pos_embed.shape[1]-1,model.pos_embed.shape[1]-1))\n",
    "cos = nn.CosineSimilarity(dim=0)\n",
    "for r in range(model.pos_embed.shape[1]-1):\n",
    "    for c in range(model.pos_embed.shape[1]-1):\n",
    "        locarray[r,c] = cos(model.pos_embed[0][r + 1],model.pos_embed[0][c + 1]).detach().cpu().numpy()\n",
    "chnarray = np.zeros((reduce_dim,reduce_dim))\n",
    "for r in range(reduce_dim):\n",
    "    for c in range(reduce_dim):\n",
    "        chnarray[r,c] = cos(model.patch_embed.channel_embed.weight[r],model.patch_embed.channel_embed.weight[c]).detach().cpu().numpy()\n",
    "ax[0].set_title(\"position embeddings\"); ax[1].set_title(\"channel embeddings\")\n",
    "ax[0].matshow(locarray); ax[1].matshow(chnarray)\n",
    "fig.tight_layout()\n",
    "if not is_local:\n",
    "    plt.savefig(f'./position_embedding_{seed}.png'); plt.close(fig) "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-04-08T10:23:36.600822900Z"
    }
   },
   "id": "115bb4e4dad34505",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
