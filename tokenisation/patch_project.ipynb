{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-04T15:59:38.040912700Z",
     "start_time": "2025-04-04T15:59:38.000494100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import openpyxl\n",
    "import h5py\n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.transforms import v2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.decomposition import PCA\n",
    "import sys\n",
    "import os\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Experiment Hyperparameters"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5c3fa5c69dc9bda7"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Ellipsis, slice(0, 965, None))\n"
     ]
    }
   ],
   "source": [
    "is_local = True # todo\n",
    "\n",
    "# Experiment\n",
    "seed = 0 if is_local else int(sys.argv[-2])\n",
    "torch.manual_seed(seed)\n",
    "image_size = 256\n",
    "\n",
    "# Data: which wavenumbers are even allowed to be considered?\n",
    "wv_start = 0\n",
    "wv_end = 965\n",
    "\n",
    "# Data loading\n",
    "test_set_fraction = 0.2\n",
    "val_set_fraction = 0.2\n",
    "batch_size= 64 # todo\n",
    "patch_dim = 25\n",
    "use_augmentation = True\n",
    "embed_dim=256\n",
    "\n",
    "# Network\n",
    "dropout_p=0.5\n",
    "\n",
    "# Training schedule\n",
    "lr = 5e-4\n",
    "l2 = 5e-2 # 5e-1\n",
    "max_iters = 5000\n",
    "pseudo_epoch = 100\n",
    "\n",
    "# dimensionality reduction parameters\n",
    "r_method = 'linear' # {'linear','pca,'fixed'}\n",
    "reduce_dim = 64 if is_local else int(sys.argv[-1]) # used only for r_method = 'pca' or 'linear'\n",
    "channels_used = np.s_[...,wv_start:wv_end] # used only when r_method = 'fixed'\n",
    "print(channels_used)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-04T15:59:38.061583600Z",
     "start_time": "2025-04-04T15:59:38.044976200Z"
    }
   },
   "id": "222959fcfa561aff",
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 228 cores\n",
      "Using 965/965 wavenumbers\n"
     ]
    }
   ],
   "source": [
    "def csf_fp(filepath):\n",
    "    return filepath.replace('D:/datasets','D:/datasets' if is_local else './')\n",
    "\n",
    "master = pd.read_excel(csf_fp(rf'D:/datasets/pcuk2023_ftir_whole_core/master_sheet.xlsx'))\n",
    "slide = master['slide'].to_numpy()\n",
    "patient_id = master['patient_id'].to_numpy()\n",
    "hdf5_filepaths = np.array([csf_fp(fp) for fp in master['hdf5_filepath']])\n",
    "annotation_filepaths = np.array([csf_fp(fp) for fp in master['annotation_filepath']])\n",
    "mask_filepaths = np.array([csf_fp(fp) for fp in master['mask_filepath']])\n",
    "wavenumbers = np.load(csf_fp(f'D:/datasets/pcuk2023_ftir_whole_core/wavenumbers.npy'))[wv_start:wv_end]\n",
    "wavenumbers_used = wavenumbers[channels_used]\n",
    "\n",
    "seq_len = len(wavenumbers_used) if r_method == 'fixed' else reduce_dim \n",
    "\n",
    "annotation_class_colors = np.array([[0,255,0],[128,0,128],[255,0,255],[0,0,255],[255,165,0],[255,0,0]])\n",
    "annotation_class_names = np.array(['epithelium_n','stroma_n','epithelium_c','stroma_c','corpora_amylacea','blood'])\n",
    "n_classes = len(annotation_class_names)\n",
    "print(f\"Loaded {len(slide)} cores\")\n",
    "print(f\"Using {len(wavenumbers_used)}/{len(wavenumbers)} wavenumbers\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-04T15:59:38.107184500Z",
     "start_time": "2025-04-04T15:59:38.052302Z"
    }
   },
   "id": "a78af96389a4cd31",
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define Datasets, Dataloaders"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aaba53c2e93ca461"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patients per data split:\n",
      "\tTRAIN: 135\n",
      "\tVAL: 49\n",
      "\tTEST: 44\n"
     ]
    }
   ],
   "source": [
    "unique_pids = np.unique(patient_id)\n",
    "pids_trainval, pids_test, _, _ = train_test_split(\n",
    "    unique_pids, np.zeros_like(unique_pids), test_size=test_set_fraction, random_state=seed)\n",
    "pids_train, pids_val, _, _ = train_test_split(\n",
    "    pids_trainval, np.zeros_like(pids_trainval), test_size=(val_set_fraction/(1-test_set_fraction)), random_state=seed)\n",
    "where_train = np.where(np.isin(patient_id,pids_train))\n",
    "where_val = np.where(np.isin(patient_id,pids_val))\n",
    "where_test = np.where(np.isin(patient_id,pids_test))\n",
    "print(f\"Patients per data split:\\n\\tTRAIN: {len(where_train[0])}\\n\\tVAL: {len(where_val[0])}\\n\\tTEST: {len(where_test[0])}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-04T15:59:38.109213Z",
     "start_time": "2025-04-04T15:59:38.099538600Z"
    }
   },
   "id": "e4655cf38851b265",
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class ftir_patching_dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,hdf5_filepaths, mask_filepaths, annotation_filepaths, channels_use,\n",
    "                 patch_dim=25, augment=True,):\n",
    "        \n",
    "        # Define data paths\n",
    "        self.hdf5_filepaths = hdf5_filepaths\n",
    "        self.mask_filepaths = mask_filepaths\n",
    "        self.annotation_filepaths = annotation_filepaths\n",
    "        self.augment = augment\n",
    "        \n",
    "        # patch dimensions\n",
    "        self.patch_dim = patch_dim\n",
    "        self.patch_minus = patch_dim //2; self.patch_plus = 1 + (patch_dim // 2)\n",
    "        self.channels = channels_use\n",
    "        \n",
    "        # class data\n",
    "        self.annotation_class_colors = annotation_class_colors\n",
    "        self.annotation_class_names = annotation_class_names\n",
    "        self.total_sampled = torch.zeros(len(self.annotation_class_colors))\n",
    "        \n",
    "        # define data augmentation pipeline\n",
    "        self.transforms = v2.Compose([\n",
    "            v2.RandomHorizontalFlip(p=0.5),\n",
    "            v2.RandomVerticalFlip(p=0.5),\n",
    "        ])\n",
    "        \n",
    "        # Open every core hdf5 file\n",
    "        self.open()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.total_pixels\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        # get patch data\n",
    "        row = self.rows[idx]\n",
    "        col = self.cols[idx]\n",
    "        cidx = self.cidxs[idx]\n",
    "        label = self.tissue_classes[idx]\n",
    "        self.total_sampled[label] += 1\n",
    "        \n",
    "        # Are dimensions of patch okay\n",
    "        idx_u = row - self.patch_minus\n",
    "        idx_d = row + self.patch_plus\n",
    "        idx_l = col - self.patch_minus\n",
    "        idx_r = col + self.patch_plus\n",
    "        pad_u = max(-idx_u,0); idx_u = max(idx_u,0)\n",
    "        pad_d = max(idx_d-image_size,0); idx_d = min(idx_d,image_size)\n",
    "        pad_l = max(-idx_l,0); idx_l = max(idx_l,0)\n",
    "        pad_r = max(idx_r-image_size,0); idx_r = min(idx_r,image_size)\n",
    "        \n",
    "        # get patch\n",
    "        patch = torch.from_numpy(\n",
    "            self.hdf5_files[cidx]['spectra'][idx_u:idx_d,idx_l:idx_r,*self.channels],\n",
    "        ).permute(2,0,1)\n",
    "        patch *= torch.from_numpy(\n",
    "            self.hdf5_files[cidx]['mask'][idx_u:idx_d,idx_l:idx_r,],\n",
    "        ).unsqueeze(0)\n",
    "        \n",
    "        # pad patch\n",
    "        patch = torch.nn.functional.pad(patch,(pad_l,pad_r,pad_u,pad_d,0,0))\n",
    "        \n",
    "        if self.augment:\n",
    "            patch = self.transforms(patch)\n",
    "        return patch,label\n",
    "\n",
    "    # split annotations from H x W x 3 to C x H x W, one/zerohot along C dimension\n",
    "    def split_annotations(self,annotations_img):\n",
    "        split = torch.zeros((len(self.annotation_class_colors),*annotations_img.shape[:-1]))\n",
    "        for c,col in enumerate(annotation_class_colors):\n",
    "            split[c,:,:] = torch.from_numpy(np.all(annotations_img == self.annotation_class_colors[c],axis=-1)) \n",
    "        return split\n",
    "    \n",
    "    # open every file \n",
    "    def open(self):\n",
    "        self.hdf5_files = []\n",
    "        self.tissue_classes = []\n",
    "        self.rows = []\n",
    "        self.cols = []\n",
    "        self.cidxs = []\n",
    "        \n",
    "        # for every core in dataset,\n",
    "        for cidx in range(0,len(self.hdf5_filepaths)):\n",
    "            # open annotations and remove edges and non-tissue px\n",
    "            annotation = self.split_annotations(cv2.imread(self.annotation_filepaths[cidx])[:,:,::-1])\n",
    "            mask = torch.from_numpy(cv2.imread(self.mask_filepaths[cidx])[:,:,1]) / 255\n",
    "            annotation *= mask\n",
    "            # for every class,\n",
    "            for cls in range(len(annotation_class_names)):\n",
    "                # get location of annotations, append to lists\n",
    "                r,c = torch.where(annotation[cls])\n",
    "                num_cls = annotation[cls].sum().int().item()\n",
    "                self.tissue_classes.extend([cls,]*num_cls)\n",
    "                self.cidxs.extend([cidx,]*num_cls)\n",
    "                self.rows.extend(r)\n",
    "                self.cols.extend(c)\n",
    "            # add open hdf5 file to list\n",
    "            self.hdf5_files.append(h5py.File(self.hdf5_filepaths[cidx],'r'))\n",
    "                \n",
    "        # construct data tensors\n",
    "        self.rows = torch.Tensor(self.rows).int()\n",
    "        self.cols = torch.Tensor(self.cols).int()\n",
    "        self.tissue_classes = torch.Tensor(self.tissue_classes).long()\n",
    "        self.cidxs = torch.Tensor(self.cidxs).int()\n",
    "        self.total_pixels = len(self.cidxs)\n",
    "\n",
    "    # close every open hdf5 file\n",
    "    def close(self):\n",
    "        for cidx in range(len(self.hdf5_files)):\n",
    "            self.hdf5_files[cidx].close()\n",
    "        self.hdf5_files = []\n",
    "        self.tissue_classes = []\n",
    "        self.xs = []\n",
    "        self.ys = []"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-04T15:59:38.118773700Z",
     "start_time": "2025-04-04T15:59:38.103811Z"
    }
   },
   "id": "a8a3aa59fbf57012",
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loader sizes:\n",
      "\ttrain: 5992\n",
      "\tval: 2013\n",
      "\ttest: 1793\n"
     ]
    }
   ],
   "source": [
    "dataset_train = ftir_patching_dataset(\n",
    "    hdf5_filepaths[where_train], mask_filepaths[where_train], annotation_filepaths[where_train], channels_used,\n",
    "    patch_dim = patch_dim, augment=use_augmentation,\n",
    ")\n",
    "dataset_val = ftir_patching_dataset(\n",
    "    hdf5_filepaths[where_val], mask_filepaths[where_val], annotation_filepaths[where_val], channels_used,\n",
    "    patch_dim = patch_dim, augment=False,\n",
    ")\n",
    "dataset_test = ftir_patching_dataset(\n",
    "    hdf5_filepaths[where_test], mask_filepaths[where_test], annotation_filepaths[where_test], channels_used,\n",
    "    patch_dim = patch_dim, augment=False,\n",
    ")\n",
    "\n",
    "# Instiantiate data loaders\n",
    "_, class_counts = np.unique(dataset_train.tissue_classes, return_counts=True)\n",
    "class_weights = 1 / class_counts\n",
    "class_weights = class_weights[dataset_train.tissue_classes]\n",
    "train_sampler = torch.utils.data.WeightedRandomSampler(class_weights, len(class_weights), replacement=True)\n",
    "\n",
    "_, class_counts = np.unique(dataset_val.tissue_classes, return_counts=True)\n",
    "class_weights = 1 / class_counts\n",
    "class_weights = class_weights[dataset_val.tissue_classes]\n",
    "val_sampler = torch.utils.data.WeightedRandomSampler(class_weights, len(class_weights), replacement=True)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size, sampler=train_sampler,drop_last=True)\n",
    "val_loader = torch.utils.data.DataLoader(dataset_val, batch_size=batch_size, sampler=val_sampler,drop_last=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset_test, batch_size=batch_size,shuffle=True,drop_last=True)\n",
    "print(f\"loader sizes:\\n\\ttrain: {len(train_loader)}\\n\\tval: {len(val_loader)}\\n\\ttest: {len(test_loader)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-04T15:59:56.651493200Z",
     "start_time": "2025-04-04T15:59:38.114641200Z"
    }
   },
   "id": "c6bebd9eeed34711",
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define dimensionality reduction method"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "abdcc80122d23428"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class LinearReduction(nn.Module):\n",
    "    def __init__(self,input_dim,reduce_dim):\n",
    "        super().__init__()\n",
    "        self.reduce_dim = reduce_dim\n",
    "        self.input_norm = nn.BatchNorm2d(input_dim)\n",
    "        self.projection = nn.Conv2d(input_dim,reduce_dim,kernel_size=1,stride=1) # todo bias = false?\n",
    "        self.projection_norm = nn.BatchNorm2d(reduce_dim)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self.projection_norm(self.projection(self.input_norm(x)))\n",
    "    \n",
    "class PCAReduce(nn.Module): # todo need to add batchnorm here too?\n",
    "    def __init__(self,reduce_dim,means,loadings):\n",
    "        super().__init__()\n",
    "        self.reduce_dim = reduce_dim\n",
    "        self.register_buffer('means', torch.from_numpy(means).float().reshape(1,-1,1,1))\n",
    "        self.register_buffer('loadings', torch.from_numpy(loadings).float())\n",
    "    \n",
    "    def forward(self,x):\n",
    "        projected = x - self.means\n",
    "        \n",
    "        b,c,h,w = projected.shape\n",
    "        projected = projected.permute(0,2,3,1).reshape(b,h*w,c)\n",
    "        projected = torch.matmul(projected, self.loadings.T)\n",
    "        projected = projected.reshape(b,h,w,self.reduce_dim).permute(0,3,1,2)\n",
    "        \n",
    "        return projected\n",
    "        \n",
    "class FixedReduction(nn.Module):\n",
    "    def __init__(self,input_dim):\n",
    "        super().__init__()\n",
    "        self.input_norm = nn.BatchNorm2d(input_dim)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self.input_norm(x)\n",
    "\n",
    "if r_method == 'pca':\n",
    "    spectral_sample = []\n",
    "    batch_samples = 0\n",
    "    for data,label in train_loader:\n",
    "        spectral_sample.append(data[...,patch_dim//2,patch_dim//2].numpy())\n",
    "        batch_samples += 1\n",
    "        if batch_samples > 10000//batch_size: break\n",
    "    spectral_sample = np.concatenate(spectral_sample,axis=0)\n",
    "    spectral_means = np.mean(spectral_sample,axis=0)\n",
    "    spectral_sample -= spectral_means\n",
    "    pca = PCA(n_components=reduce_dim)\n",
    "    pca.fit(spectral_sample)\n",
    "    spectral_loadings = pca.components_"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-04T15:59:56.658314200Z",
     "start_time": "2025-04-04T15:59:56.654582800Z"
    }
   },
   "id": "e49ecda26ba39e97",
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define Model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f550708e9f4c74c5"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class LearnablePositionEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, dropout: float = 0.0, sequence_length: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.pe = nn.Parameter(torch.tensor(np.random.normal(loc=0.0, scale=0.02, size=(1, sequence_length, d_model)),dtype=torch.float32))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x + self.pe\n",
    "        return self.dropout(x)\n",
    "\n",
    "class patch25_transformer(nn.Module):\n",
    "    def __init__(self,input_dim,reduce_dim,n_classes,embed_dim):\n",
    "        super().__init__()\n",
    "        # input processing and dimensionality reduction\n",
    "        if r_method == 'pca':\n",
    "            self.input_processing = PCAReduce(reduce_dim,spectral_means,spectral_loadings)\n",
    "        elif r_method == 'fixed':\n",
    "            self.input_processing = FixedReduction(input_dim)\n",
    "        elif r_method == 'linear':\n",
    "            self.input_processing = LinearReduction(input_dim,reduce_dim)\n",
    "            \n",
    "        self.d_model = embed_dim\n",
    "        self.cls_token = nn.Parameter(torch.normal(torch.zeros((1,1,self.d_model)),0.02))\n",
    "            \n",
    "        self.patch_project = nn.Conv3d(\n",
    "                1,\n",
    "                embed_dim,\n",
    "                kernel_size=(1,patch_dim,patch_dim),\n",
    "        )\n",
    "            \n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=self.d_model,\n",
    "            nhead=16,\n",
    "            dim_feedforward=256,\n",
    "            activation=\"gelu\",\n",
    "            batch_first=True)\n",
    "        self.encoder_stack = nn.TransformerEncoder(\n",
    "            encoder_layer=self.encoder_layer,\n",
    "            num_layers=6)\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.d_model,n_classes),\n",
    "        )\n",
    "        \n",
    "        self.pe = LearnablePositionEncoding(\n",
    "            d_model=self.d_model,\n",
    "            dropout=0,\n",
    "            sequence_length=seq_len + 1\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.input_processing(x)\n",
    "        x = self.patch_project(x.unsqueeze(1)) # X -> B x 1 x C x H x W -> B x K x C x 1 x 1 -> \n",
    "        x = x.squeeze().permute(0,2,1) # X -> B x C x K\n",
    "        x = torch.cat([self.cls_token.expand(x.shape[0],-1,-1),x],dim=1)\n",
    "        x_pe = self.pe(x)\n",
    "        x_tr = self.encoder_stack(x_pe)\n",
    "        logits = self.classifier(x_tr[:, 0])\n",
    "        return logits\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-04T15:59:56.663759800Z",
     "start_time": "2025-04-04T15:59:56.661360800Z"
    }
   },
   "id": "33535f38f1458959",
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fusion_model with 3.182M params composed of:\n"
     ]
    }
   ],
   "source": [
    "model = patch25_transformer(\n",
    "    input_dim=len(wavenumbers_used),\n",
    "    reduce_dim=len(wavenumbers_used) if r_method == 'fixed' else reduce_dim,\n",
    "    n_classes=n_classes,\n",
    "    embed_dim=embed_dim,\n",
    ")\n",
    "\n",
    "print(f\"fusion_model with {sum(p.numel() for p in model.parameters() if p.requires_grad) / 1e6:.3f}M params composed of:\")\n",
    "model = model.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-04T15:59:56.682086600Z",
     "start_time": "2025-04-04T15:59:56.665796900Z"
    }
   },
   "id": "a10ccd9a5ba9b9e9",
   "execution_count": 25
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training Loop"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4dfdaf2ad556fa46"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr,weight_decay=l2)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=500, threshold=0.01, cooldown=250)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-04T15:59:56.686223200Z",
     "start_time": "2025-04-04T15:59:56.683083700Z"
    }
   },
   "id": "50a59f2ddf1e3f0d",
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "training_losses,validation_losses = [],[]\n",
    "training_accs,validation_accs = [],[]\n",
    "training_f1ms,validation_f1ms = [],[]\n",
    "training_f1s,validation_f1s = [],[]\n",
    "lr_decreases = []\n",
    "current_iters = 0\n",
    "best_val_f1 = 0\n",
    "best_val_iter = 0\n",
    "stop_training=False"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-04T15:59:56.689470200Z",
     "start_time": "2025-04-04T15:59:56.687253Z"
    }
   },
   "id": "51f82808428ba870",
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "while current_iters < max_iters:\n",
    "    for (bidx, (data, label)) in enumerate(train_loader):\n",
    "        data = data.to(device); label = label.to(device)\n",
    "        \n",
    "        # Push through model\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = loss_fn(out, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Append log arrays\n",
    "        training_losses.append(loss.item())\n",
    "        pred = out.argmax(dim=1).detach().cpu().numpy()\n",
    "        actual = label.cpu().numpy()\n",
    "        training_accs.append(accuracy_score(actual,pred))\n",
    "        training_f1ms.append(f1_score(actual, pred, average='macro'))\n",
    "        training_f1s.append(f1_score(actual, pred, average=None, labels=np.arange(0,n_classes),zero_division=0))\n",
    "        \n",
    "        # Do validation cycle\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            # load data\n",
    "            data, label = next(iter(val_loader))\n",
    "            data = data.to(device); label = label.to(device)\n",
    "            \n",
    "            # Push through model\n",
    "            out = model(data)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = loss_fn(out, label)\n",
    "            \n",
    "            # Append log arrays\n",
    "            validation_losses.append(loss.item())\n",
    "            pred = out.argmax(dim=1).detach().cpu().numpy()\n",
    "            actual = label.cpu().numpy()\n",
    "            validation_accs.append(accuracy_score(actual,pred))\n",
    "            validation_f1ms.append(f1_score(actual, pred, average='macro'))\n",
    "            validation_f1s.append(f1_score(actual, pred, average=None, labels=np.arange(0,n_classes),zero_division=0))\n",
    "        \n",
    "        # Print training statistics every N iters\n",
    "        if current_iters % pseudo_epoch == 0:\n",
    "            print(f\"ON ITER: {current_iters}, metrics for last {pseudo_epoch} iters:\")\n",
    "            print(f\"TRAIN --- | Loss: {np.mean(training_losses[-pseudo_epoch:]):.4} | OA: {np.mean(training_accs[-pseudo_epoch:]):.4} | F1M: {np.mean(training_f1ms[-pseudo_epoch:]):.4f}\")\n",
    "            print(f\"VAL ----- | Loss: {np.mean(validation_losses[-pseudo_epoch:]):.4} | OA: {np.mean(validation_accs[-pseudo_epoch:]):.4} | F1M: {np.mean(validation_f1ms[-pseudo_epoch:]):.4f}\")\n",
    "        \n",
    "        # If performance on validation set best so far, save model\n",
    "        if np.mean(validation_f1ms[-pseudo_epoch:]) > best_val_f1:\n",
    "            best_val_f1 = np.mean(validation_f1ms[-pseudo_epoch:])\n",
    "            best_val_iter = current_iters\n",
    "            if not is_local:\n",
    "                torch.save(model.state_dict(), rf'./model_weights_{seed}.pt')\n",
    "        \n",
    "        # Step the scheduler based on the validation set performance\n",
    "        current_iters += 1\n",
    "        if current_iters > max_iters: \n",
    "            stop_training = True\n",
    "            break\n",
    "        if current_iters > pseudo_epoch:\n",
    "            scheduler.step(np.mean(validation_f1ms[-pseudo_epoch:]))\n",
    "            new_lr = optimizer.param_groups[0]['lr']\n",
    "            if new_lr != lr:\n",
    "                print(f\"Val f1 plateaued, lr {lr} -> {new_lr}\")\n",
    "                lr = new_lr\n",
    "                lr_decreases.append(current_iters)\n",
    "                if len(lr_decreases) >= 2: \n",
    "                    stop_training = True\n",
    "                    print(\"Val f1 decreased twice, ending training early\")\n",
    "                    break\n",
    "    if stop_training: break\n",
    "\n",
    "training_losses = np.array(training_losses); validation_losses = np.array(validation_losses)\n",
    "training_accs = np.array(training_accs); validation_accs = np.array(validation_accs)\n",
    "training_f1ms = np.array(training_f1ms); validation_f1ms = np.array(validation_f1ms)\n",
    "training_f1s = np.stack(training_f1s,axis=0); validation_f1s = np.stack(validation_f1s,axis=0)\n",
    "print(f\"Training complete after {current_iters} iterations\\n\\ttotal samples       :    {current_iters*batch_size}\\n\\t -=-=-=-=-=-=-=-=-=-=-=-=-=-\")\n",
    "for cls_idx, samples_loaded in enumerate(dataset_train.total_sampled.numpy()):\n",
    "    print(f\"\\t{annotation_class_names[cls_idx]}{(20-len(annotation_class_names[cls_idx])) * ' '}:    {int(samples_loaded)}\")\n",
    "print(f\"Metrics for final {pseudo_epoch} iterations:\")\n",
    "print(f\"TRAIN --- | Loss: {training_losses[-pseudo_epoch:].mean():.4f} | OA: {training_accs[-pseudo_epoch:].mean():.4f} | f1: {training_f1ms[-pseudo_epoch:].mean():.4f}\")\n",
    "print(f\"VAL ----- | Loss: {validation_losses[-pseudo_epoch:].mean():.4f} | OA: {validation_accs[-pseudo_epoch:].mean():.4f} | f1: {validation_f1ms[-pseudo_epoch:].mean():.4f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2025-04-04T15:59:56.694665800Z"
    }
   },
   "id": "7c6adbd14fd799f6",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test Loop"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "307cf693fb06cb7e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "running_loss_test = 0\n",
    "test_preds, test_targets = [], []\n",
    "\n",
    "if not is_local:\n",
    "    model.load_state_dict(torch.load(rf'./model_weights_{seed}.pt', weights_only=True))\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (data, label) in enumerate(test_loader):\n",
    "        print(f\"Iter: {batch_idx}/{len(test_loader)}\",end=\"\\r\")        \n",
    "        data = data.to(device)\n",
    "        label = label.to(device)\n",
    "\n",
    "        # Push through model\n",
    "        out = model(data)\n",
    "        loss = loss_fn(out, label)\n",
    "\n",
    "        # Calculate metrics\n",
    "        running_loss_test += loss.cpu().item() \n",
    "        pred = out.argmax(dim=1).detach().cpu().numpy()\n",
    "        actual = label.cpu().numpy()\n",
    "        test_preds.extend(pred)\n",
    "        test_targets.extend(actual)\n",
    "\n",
    "test_targets = np.array(test_targets); test_preds = np.array(test_preds)\n",
    "test_loss = running_loss_test / batch_idx\n",
    "test_acc = accuracy_score(test_targets, test_preds)\n",
    "test_f1m = f1_score(test_targets, test_preds, average='macro')\n",
    "test_f1 = f1_score(test_targets, test_preds, average=None)\n",
    "\n",
    "print(\"Metrics on entire testing set:\")\n",
    "print(f\"TEST ---- | Loss: {test_loss:.4f} | OA: {test_acc:.4f} | f1: {test_f1m:.4f}\")\n",
    "for cls_idx, f1 in enumerate(test_f1):\n",
    "    print(f\"{annotation_class_names[cls_idx]}{(20 - len(annotation_class_names[cls_idx])) * ' '} : {f1:.4f}\")\n",
    "print(\"Total samples loaded for each class during TESTING\")\n",
    "for cls_idx, samples_loaded in enumerate(dataset_test.total_sampled.numpy()):\n",
    "    print(f\"{annotation_class_names[cls_idx]}{(20-len(annotation_class_names[cls_idx])) * ' '}:    {int(samples_loaded)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "242a34c1a6fe3be8",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(8,8))\n",
    "locarray = np.zeros((seq_len,seq_len))\n",
    "cos = nn.CosineSimilarity(dim=0)\n",
    "for r in range(seq_len):\n",
    "    for c in range(seq_len):\n",
    "        locarray[r,c] = cos(model.pe.pe[0][r + 1],model.pe.pe[0][c + 1]).detach().cpu().numpy()\n",
    "fig.suptitle(\"Cosine similarity of position embeddings\")\n",
    "ax.matshow(locarray)\n",
    "fig.tight_layout()\n",
    "if not is_local:\n",
    "    plt.savefig(f'./position_embedding_{seed}.png'); plt.close(fig)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "88f9ba9a718fd48c",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "67ff17a2ebecf4ae"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def moving_average(a, n=3): # https://stackoverflow.com/questions/14313510/how-to-calculate-rolling-moving-average-using-python-numpy-scipy\n",
    "    a = np.pad(a, ((n-1)//2,(n-1)//2 + ((n-1) % 2)), mode='edge')\n",
    "    ret = np.cumsum(a, dtype=float)\n",
    "    ret[n:] = ret[n:] - ret[:-n]\n",
    "    return ret[n - 1:] / n\n",
    "\n",
    "fig,ax = plt.subplots(1,3,figsize=(16,5))\n",
    "ax[0].plot(np.arange(0,len(moving_average(training_losses,n=1))),moving_average(training_losses,n=1),alpha=0.3,color='cornflowerblue')\n",
    "ax[0].plot(np.arange(0,len(moving_average(training_losses,n=50))),moving_average(training_losses,n=50),alpha=1,color='cornflowerblue',label=\"train\")\n",
    "ax[0].plot(np.arange(0,len(moving_average(validation_losses,n=1))),moving_average(validation_losses,n=1),alpha=0.3,color='orange')\n",
    "ax[0].plot(np.arange(0,len(moving_average(validation_losses,n=50))),moving_average(validation_losses,n=50),alpha=1,color='orange',label=\"validation\")\n",
    "ax[0].scatter(current_iters,test_loss,color='green',label=\"test\",marker=\"x\")\n",
    "ax[0].set_title(\"Loss\"); ax[0].legend()\n",
    "\n",
    "ax[1].plot(np.arange(0,len(moving_average(training_accs,n=1))),moving_average(training_accs,n=1),alpha=0.3,color='cornflowerblue')\n",
    "ax[1].plot(np.arange(0,len(moving_average(training_accs,n=50))),moving_average(training_accs,n=50),alpha=1,color='cornflowerblue',label=\"train\")\n",
    "ax[1].plot(np.arange(0,len(moving_average(validation_accs,n=1))),moving_average(validation_accs,n=1),alpha=0.3,color='orange')\n",
    "ax[1].plot(np.arange(0,len(moving_average(validation_accs,n=50))),moving_average(validation_accs,n=50),alpha=1,color='orange',label=\"validation\")\n",
    "ax[1].scatter(current_iters,test_acc,color='green',label=\"test\",marker=\"x\")\n",
    "ax[1].set_title(\"Accuracy\"); ax[1].legend()\n",
    "\n",
    "ax[2].plot(np.arange(0,len(moving_average(training_f1ms,n=1))),moving_average(training_f1ms,n=1),alpha=0.3,color='cornflowerblue')\n",
    "ax[2].plot(np.arange(0,len(moving_average(training_f1ms,n=50))),moving_average(training_f1ms,n=50),alpha=1,color='cornflowerblue',label=\"train\")\n",
    "ax[2].plot(np.arange(0,len(moving_average(validation_f1ms,n=1))),moving_average(validation_f1ms,n=1),alpha=0.3,color='orange')\n",
    "ax[2].plot(np.arange(0,len(moving_average(validation_f1ms,n=50))),moving_average(validation_f1ms,n=50),alpha=1,color='orange',label=\"validation\")\n",
    "ax[2].scatter(current_iters,test_f1m,color='green',label=\"test\",marker=\"x\")\n",
    "ax[2].set_title(\"Macro F1 Score\"); ax[2].legend()\n",
    "\n",
    "ax[0].axvline(x=best_val_iter, ymin=0, ymax=1, color='red',alpha=0.3)\n",
    "ax[1].axvline(x=best_val_iter, ymin=0, ymax=1, color='red',alpha=0.3)\n",
    "ax[2].axvline(x=best_val_iter, ymin=0, ymax=1, color='red',alpha=0.3)\n",
    "\n",
    "for lrd in lr_decreases:\n",
    "    ax[0].axvline(x=lrd, ymin=0, ymax=1, color='grey',alpha=0.3)\n",
    "    ax[1].axvline(x=lrd, ymin=0, ymax=1, color='grey',alpha=0.3)\n",
    "    ax[2].axvline(x=lrd, ymin=0, ymax=1, color='grey',alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "if not is_local:\n",
    "    plt.savefig(f'./loss_curve_{seed}.png'); plt.close(fig)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "5aa6f4abb81d43a8",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "training_f1s = np.stack(training_f1s,axis=0)\n",
    "validation_f1s = np.stack(validation_f1s,axis=0)\n",
    "fig,ax = plt.subplots(2,3,figsize=(15,5)); ax = ax.flatten()\n",
    "for cls in range(n_classes):\n",
    "    ax[cls].plot(np.arange(0,len(moving_average(training_f1s[:,cls],n=1))),moving_average(training_f1s[:,cls],n=1),alpha=0.3,color='k')\n",
    "    ax[cls].plot(np.arange(0,len(moving_average(training_f1s[:,cls],n=50))),moving_average(training_f1s[:,cls],n=50),alpha=1,color='k',label=\"train\")\n",
    "    ax[cls].plot(np.arange(0,len(moving_average(validation_f1s[:,cls],n=1))),moving_average(validation_f1s[:,cls],n=1),alpha=0.3,color=annotation_class_colors[cls]/255)\n",
    "    ax[cls].plot(np.arange(0,len(moving_average(validation_f1s[:,cls],n=50))),moving_average(validation_f1s[:,cls],n=50),alpha=1,color=annotation_class_colors[cls]/255, label=\"validation\")\n",
    "    ax[cls].scatter(current_iters,test_f1[cls],color=annotation_class_colors[cls]/255,label=\"test\",marker=\"x\")\n",
    "    ax[cls].set_ylim(ymin=0,ymax=1)\n",
    "    for lrd in lr_decreases:\n",
    "        ax[cls].axvline(x=lrd, ymin=0, ymax=1, color='grey',alpha=0.5)\n",
    "    ax[cls].axvline(x=best_val_iter, ymin=0, ymax=1, color='red',alpha=0.3)\n",
    "fig.suptitle(\"Class-specific F1 scores\")\n",
    "plt.tight_layout()\n",
    "if not is_local:\n",
    "    plt.savefig(f'./loss_curve_individual_{seed}.png'); plt.close(fig)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "60c4c1d2914ee636",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Finish experiment"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5cd7e4042615b52"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "if not is_local:\n",
    "    model = model.cpu()\n",
    "    torch.save(model.state_dict(), rf'./model_weights_{seed}.pt')"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "b8248f10250d5d36",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Read existing results file\n",
    "if not is_local:\n",
    "    if os.path.isfile('results.txt'):\n",
    "        f = open('results.txt','r')\n",
    "        lines = f.readlines()\n",
    "        f.close()\n",
    "    else: \n",
    "        lines = [x+', \\n' for x in['seed',*annotation_class_names,'overall_acc','macro_f1']]\n",
    "        \n",
    "    # Process files\n",
    "    lines[0] = lines[0].replace('\\n',str(seed) + ', \\n')\n",
    "    for cls in range(n_classes):\n",
    "        lines[cls+1] = lines[cls+1].replace('\\n',str(test_f1[cls]) + ', \\n' )\n",
    "    lines[n_classes+1] = lines[n_classes+1].replace('\\n',str(test_acc) + ', \\n')\n",
    "    lines[n_classes+2] = lines[n_classes+2].replace('\\n',str(test_f1m) + ', \\n')\n",
    "    \n",
    "    f = open('results.txt','w')\n",
    "    f.write(''.join(lines))\n",
    "    f.close()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "bcb3c45e041764e8",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "dataset_train.close()\n",
    "dataset_val.close()\n",
    "dataset_test.close()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "10eaa319e3f02f12"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
