{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-04T15:30:58.074545100Z",
     "start_time": "2025-04-04T15:30:54.518353700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import openpyxl\n",
    "import h5py\n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.transforms import v2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.decomposition import PCA\n",
    "import sys\n",
    "import os\n",
    "import random \n",
    "import warnings\n",
    "import math\n",
    "from functools import partial\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Experiment Hyperparameters"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5c3fa5c69dc9bda7"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Ellipsis, slice(None, None, None))\n"
     ]
    }
   ],
   "source": [
    "is_local = True # todo\n",
    "\n",
    "# Experiment\n",
    "seed = 1000 if is_local else int(sys.argv[-2])\n",
    "torch.manual_seed(seed)\n",
    "image_size = 256\n",
    "\n",
    "# Data: which wavenumbers are even allowed to be considered?\n",
    "wv_start = 0\n",
    "wv_end = 965\n",
    "\n",
    "# Data loading\n",
    "test_set_fraction = 0.2\n",
    "val_set_fraction = 0.2\n",
    "batch_size= 64\n",
    "patch_dim = 101\n",
    "use_augmentation = True\n",
    "\n",
    "# Network\n",
    "dropout_p=0.5\n",
    "\n",
    "# Training schedule\n",
    "lr = 5e-4\n",
    "l2 = 5e-2\n",
    "max_iters = 5000\n",
    "pseudo_epoch = 100\n",
    "\n",
    "# dimensionality reduction parameters\n",
    "r_method = 'linear' # {'linear','pca,'fixed'}\n",
    "reduce_dim = 16 if is_local else int(sys.argv[-1]) # used only for r_method = 'pca' or 'linear'\n",
    "channels_used = np.s_[...,:] # used only when r_method = 'fixed'\n",
    "print(channels_used)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-04T15:30:58.080731400Z",
     "start_time": "2025-04-04T15:30:58.072421200Z"
    }
   },
   "id": "222959fcfa561aff",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 228 cores\n",
      "Using 965/965 wavenumbers\n"
     ]
    }
   ],
   "source": [
    "def csf_fp(filepath):\n",
    "    return filepath.replace('D:/datasets','D:/datasets' if is_local else './')\n",
    "\n",
    "master = pd.read_excel(csf_fp(rf'D:/datasets/pcuk2023_ftir_whole_core/master_sheet.xlsx'))\n",
    "slide = master['slide'].to_numpy()\n",
    "patient_id = master['patient_id'].to_numpy()\n",
    "hdf5_filepaths = np.array([csf_fp(fp) for fp in master['hdf5_filepath']])\n",
    "annotation_filepaths = np.array([csf_fp(fp) for fp in master['annotation_filepath']])\n",
    "mask_filepaths = np.array([csf_fp(fp) for fp in master['mask_filepath']])\n",
    "wavenumbers = np.load(csf_fp(f'D:/datasets/pcuk2023_ftir_whole_core/wavenumbers.npy'))[wv_start:wv_end]\n",
    "wavenumbers_used = wavenumbers[channels_used]\n",
    "\n",
    "annotation_class_colors = np.array([[0,255,0],[128,0,128],[255,0,255],[0,0,255],[255,165,0],[255,0,0]])\n",
    "annotation_class_names = np.array(['epithelium_n','stroma_n','epithelium_c','stroma_c','corpora_amylacea','blood'])\n",
    "n_classes = len(annotation_class_names)\n",
    "print(f\"Loaded {len(slide)} cores\")\n",
    "print(f\"Using {len(wavenumbers_used)}/{len(wavenumbers)} wavenumbers\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-04T15:30:58.122999500Z",
     "start_time": "2025-04-04T15:30:58.077606200Z"
    }
   },
   "id": "a78af96389a4cd31",
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define Datasets, Dataloaders"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aaba53c2e93ca461"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patients per data split:\n",
      "\tTRAIN: 130\n",
      "\tVAL: 51\n",
      "\tTEST: 47\n"
     ]
    }
   ],
   "source": [
    "unique_pids = np.unique(patient_id)\n",
    "pids_trainval, pids_test, _, _ = train_test_split(\n",
    "    unique_pids, np.zeros_like(unique_pids), test_size=test_set_fraction, random_state=seed)\n",
    "pids_train, pids_val, _, _ = train_test_split(\n",
    "    pids_trainval, np.zeros_like(pids_trainval), test_size=(val_set_fraction/(1-test_set_fraction)), random_state=seed)\n",
    "where_train = np.where(np.isin(patient_id,pids_train))\n",
    "where_val = np.where(np.isin(patient_id,pids_val))\n",
    "where_test = np.where(np.isin(patient_id,pids_test))\n",
    "print(f\"Patients per data split:\\n\\tTRAIN: {len(where_train[0])}\\n\\tVAL: {len(where_val[0])}\\n\\tTEST: {len(where_test[0])}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-04T15:30:58.123528500Z",
     "start_time": "2025-04-04T15:30:58.111396300Z"
    }
   },
   "id": "e4655cf38851b265",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class ftir_patching_dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,hdf5_filepaths, mask_filepaths, annotation_filepaths, channels_use,\n",
    "                 patch_dim=25, augment=True,):\n",
    "        \n",
    "        # Define data paths\n",
    "        self.hdf5_filepaths = hdf5_filepaths\n",
    "        self.mask_filepaths = mask_filepaths\n",
    "        self.annotation_filepaths = annotation_filepaths\n",
    "        self.augment = augment\n",
    "        \n",
    "        # patch dimensions\n",
    "        self.patch_dim = patch_dim\n",
    "        self.patch_minus = patch_dim //2; self.patch_plus = 1 + (patch_dim // 2)\n",
    "        self.channels = channels_use\n",
    "        \n",
    "        # class data\n",
    "        self.annotation_class_colors = annotation_class_colors\n",
    "        self.annotation_class_names = annotation_class_names\n",
    "        self.total_sampled = torch.zeros(len(self.annotation_class_colors))\n",
    "        \n",
    "        # define data augmentation pipeline\n",
    "        self.transforms = v2.Compose([\n",
    "            v2.RandomHorizontalFlip(p=0.5),\n",
    "            v2.RandomVerticalFlip(p=0.5),\n",
    "        ])\n",
    "        \n",
    "        # Open every core hdf5 file\n",
    "        self.open()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.total_pixels\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        # get patch data\n",
    "        row = self.rows[idx]\n",
    "        col = self.cols[idx]\n",
    "        cidx = self.cidxs[idx]\n",
    "        label = self.tissue_classes[idx]\n",
    "        self.total_sampled[label] += 1\n",
    "        \n",
    "        # Are dimensions of patch okay\n",
    "        idx_u = row - self.patch_minus\n",
    "        idx_d = row + self.patch_plus\n",
    "        idx_l = col - self.patch_minus\n",
    "        idx_r = col + self.patch_plus\n",
    "        pad_u = max(-idx_u,0); idx_u = max(idx_u,0)\n",
    "        pad_d = max(idx_d-image_size,0); idx_d = min(idx_d,image_size)\n",
    "        pad_l = max(-idx_l,0); idx_l = max(idx_l,0)\n",
    "        pad_r = max(idx_r-image_size,0); idx_r = min(idx_r,image_size)\n",
    "        \n",
    "        # get patch\n",
    "        patch = torch.from_numpy(\n",
    "            self.hdf5_files[cidx]['spectra'][idx_u:idx_d,idx_l:idx_r,*self.channels],\n",
    "        ).permute(2,0,1)\n",
    "        patch *= torch.from_numpy(\n",
    "            self.hdf5_files[cidx]['mask'][idx_u:idx_d,idx_l:idx_r,],\n",
    "        ).unsqueeze(0)\n",
    "        \n",
    "        # pad patch\n",
    "        patch = torch.nn.functional.pad(patch,(pad_l,pad_r,pad_u,pad_d,0,0))\n",
    "        \n",
    "        if self.augment:\n",
    "            patch = self.transforms(patch)\n",
    "        return patch,label\n",
    "\n",
    "    # split annotations from H x W x 3 to C x H x W, one/zerohot along C dimension\n",
    "    def split_annotations(self,annotations_img):\n",
    "        split = torch.zeros((len(self.annotation_class_colors),*annotations_img.shape[:-1]))\n",
    "        for c,col in enumerate(annotation_class_colors):\n",
    "            split[c,:,:] = torch.from_numpy(np.all(annotations_img == self.annotation_class_colors[c],axis=-1)) \n",
    "        return split\n",
    "    \n",
    "    # open every file \n",
    "    def open(self):\n",
    "        self.hdf5_files = []\n",
    "        self.tissue_classes = []\n",
    "        self.rows = []\n",
    "        self.cols = []\n",
    "        self.cidxs = []\n",
    "        \n",
    "        # for every core in dataset,\n",
    "        for cidx in range(0,len(self.hdf5_filepaths)):\n",
    "            # open annotations and remove edges and non-tissue px\n",
    "            annotation = self.split_annotations(cv2.imread(self.annotation_filepaths[cidx])[:,:,::-1])\n",
    "            mask = torch.from_numpy(cv2.imread(self.mask_filepaths[cidx])[:,:,1]) / 255\n",
    "            annotation *= mask\n",
    "            # for every class,\n",
    "            for cls in range(len(annotation_class_names)):\n",
    "                # get location of annotations, append to lists\n",
    "                r,c = torch.where(annotation[cls])\n",
    "                num_cls = annotation[cls].sum().int().item()\n",
    "                self.tissue_classes.extend([cls,]*num_cls)\n",
    "                self.cidxs.extend([cidx,]*num_cls)\n",
    "                self.rows.extend(r)\n",
    "                self.cols.extend(c)\n",
    "            # add open hdf5 file to list\n",
    "            self.hdf5_files.append(h5py.File(self.hdf5_filepaths[cidx],'r'))\n",
    "                \n",
    "        # construct data tensors\n",
    "        self.rows = torch.Tensor(self.rows).int()\n",
    "        self.cols = torch.Tensor(self.cols).int()\n",
    "        self.tissue_classes = torch.Tensor(self.tissue_classes).long()\n",
    "        self.cidxs = torch.Tensor(self.cidxs).int()\n",
    "        self.total_pixels = len(self.cidxs)\n",
    "\n",
    "    # close every open hdf5 file\n",
    "    def close(self):\n",
    "        for cidx in range(len(self.hdf5_files)):\n",
    "            self.hdf5_files[cidx].close()\n",
    "        self.hdf5_files = []\n",
    "        self.tissue_classes = []\n",
    "        self.xs = []\n",
    "        self.ys = []"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-04T15:30:58.132754500Z",
     "start_time": "2025-04-04T15:30:58.121130Z"
    }
   },
   "id": "a8a3aa59fbf57012",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loader sizes:\n",
      "\ttrain: 5460\n",
      "\tval: 2221\n",
      "\ttest: 2117\n"
     ]
    }
   ],
   "source": [
    "dataset_train = ftir_patching_dataset(\n",
    "    hdf5_filepaths[where_train], mask_filepaths[where_train], annotation_filepaths[where_train], channels_used,\n",
    "    patch_dim = patch_dim, augment=use_augmentation,\n",
    ")\n",
    "dataset_val = ftir_patching_dataset(\n",
    "    hdf5_filepaths[where_val], mask_filepaths[where_val], annotation_filepaths[where_val], channels_used,\n",
    "    patch_dim = patch_dim, augment=False,\n",
    ")\n",
    "dataset_test = ftir_patching_dataset(\n",
    "    hdf5_filepaths[where_test], mask_filepaths[where_test], annotation_filepaths[where_test], channels_used,\n",
    "    patch_dim = patch_dim, augment=False,\n",
    ")\n",
    "\n",
    "# Instiantiate data loaders\n",
    "_, class_counts = np.unique(dataset_train.tissue_classes, return_counts=True)\n",
    "class_weights = 1 / class_counts\n",
    "class_weights = class_weights[dataset_train.tissue_classes]\n",
    "train_sampler = torch.utils.data.WeightedRandomSampler(class_weights, len(class_weights), replacement=True)\n",
    "\n",
    "_, class_counts = np.unique(dataset_val.tissue_classes, return_counts=True)\n",
    "class_weights = 1 / class_counts\n",
    "class_weights = class_weights[dataset_val.tissue_classes]\n",
    "val_sampler = torch.utils.data.WeightedRandomSampler(class_weights, len(class_weights), replacement=True)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size, sampler=train_sampler,drop_last=True)\n",
    "val_loader = torch.utils.data.DataLoader(dataset_val, batch_size=batch_size, sampler=val_sampler,drop_last=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset_test, batch_size=batch_size,shuffle=True,drop_last=True)\n",
    "print(f\"loader sizes:\\n\\ttrain: {len(train_loader)}\\n\\tval: {len(val_loader)}\\n\\ttest: {len(test_loader)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-04T15:31:02.260461Z",
     "start_time": "2025-04-04T15:30:58.126561400Z"
    }
   },
   "id": "c6bebd9eeed34711",
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define dimensionality reduction method"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "27b5434becc5286e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class LinearReduction(nn.Module):\n",
    "    def __init__(self,input_dim,reduce_dim):\n",
    "        super().__init__()\n",
    "        self.reduce_dim = reduce_dim\n",
    "        self.input_norm = nn.BatchNorm2d(input_dim)\n",
    "        self.projection = nn.Conv2d(input_dim,reduce_dim,kernel_size=1,stride=1)\n",
    "        self.projection_norm = nn.BatchNorm2d(reduce_dim)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self.projection_norm(self.projection(self.input_norm(x)))\n",
    "    \n",
    "class PCAReduce(nn.Module):\n",
    "    def __init__(self,reduce_dim,means,loadings):\n",
    "        super().__init__()\n",
    "        self.reduce_dim = reduce_dim\n",
    "        self.register_buffer('means', torch.from_numpy(means).float().reshape(1,-1,1,1))\n",
    "        self.register_buffer('loadings', torch.from_numpy(loadings).float())\n",
    "    \n",
    "    def forward(self,x):\n",
    "        projected = x - self.means\n",
    "        \n",
    "        b,c,h,w = projected.shape\n",
    "        projected = projected.permute(0,2,3,1).reshape(b,h*w,c)\n",
    "        projected = torch.matmul(projected, self.loadings.T)\n",
    "        projected = projected.reshape(b,h,w,self.reduce_dim).permute(0,3,1,2)\n",
    "        \n",
    "        return projected\n",
    "        \n",
    "class FixedReduction(nn.Module):\n",
    "    def __init__(self,input_dim):\n",
    "        super().__init__()\n",
    "        self.input_norm = nn.BatchNorm2d(input_dim)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self.input_norm(x)\n",
    "\n",
    "if r_method == 'pca':\n",
    "    spectral_sample = []\n",
    "    batch_samples = 0\n",
    "    for data, annotations, mask, has_annotations in train_loader:\n",
    "        where = torch.where(has_annotations[0] == 1)\n",
    "        ridxs = torch.randperm(where[0].shape[0])[:100]\n",
    "        spectral_sample.append(data[:, :, where[0][ridxs],where[1][ridxs]].permute(0,2,1).flatten(0,1).numpy())\n",
    "        batch_samples += 1\n",
    "        if batch_samples > 10: break\n",
    "    spectral_sample = np.concatenate(spectral_sample,axis=0)\n",
    "    spectral_means = np.mean(spectral_sample,axis=0)\n",
    "    spectral_sample -= spectral_means\n",
    "    pca = PCA(n_components=reduce_dim)\n",
    "    pca.fit(spectral_sample)\n",
    "    spectral_loadings = pca.components_\n",
    "    \n",
    "if r_method == 'pca':\n",
    "    input_processing = PCAReduce(reduce_dim,spectral_means,spectral_loadings)\n",
    "elif r_method == 'fixed':\n",
    "    input_processing = FixedReduction(input_dim=len(wavenumbers_used))\n",
    "elif r_method == 'linear':\n",
    "    input_processing = LinearReduction(input_dim=len(wavenumbers_used),reduce_dim=reduce_dim)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-04T15:31:02.266857500Z",
     "start_time": "2025-04-04T15:31:02.263593700Z"
    }
   },
   "id": "b5f1cfe137e09f85",
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define Model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f550708e9f4c74c5"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def _no_grad_trunc_normal_(tensor, mean, std, a, b):\n",
    "    # Cut & paste from PyTorch official master until it's in a few official releases - RW\n",
    "    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf\n",
    "    def norm_cdf(x):\n",
    "        # Computes standard normal cumulative distribution function\n",
    "        return (1.0 + math.erf(x / math.sqrt(2.0))) / 2.0\n",
    "\n",
    "    if (mean < a - 2 * std) or (mean > b + 2 * std):\n",
    "        warnings.warn(\n",
    "            \"mean is more than 2 std from [a, b] in nn.init.trunc_normal_. \"\n",
    "            \"The distribution of values may be incorrect.\",\n",
    "            stacklevel=2,\n",
    "        )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Values are generated by using a truncated uniform distribution and\n",
    "        # then using the inverse CDF for the normal distribution.\n",
    "        # Get upper and lower cdf values\n",
    "        l = norm_cdf((a - mean) / std)\n",
    "        u = norm_cdf((b - mean) / std)\n",
    "\n",
    "        # Uniformly fill tensor with values from [l, u], then translate to\n",
    "        # [2l-1, 2u-1].\n",
    "        tensor.uniform_(2 * l - 1, 2 * u - 1)\n",
    "\n",
    "        # Use inverse cdf transform for normal distribution to get truncated\n",
    "        # standard normal\n",
    "        tensor.erfinv_()\n",
    "\n",
    "        # Transform to proper mean, std\n",
    "        tensor.mul_(std * math.sqrt(2.0))\n",
    "        tensor.add_(mean)\n",
    "\n",
    "        # Clamp to ensure it's in the proper range\n",
    "        tensor.clamp_(min=a, max=b)\n",
    "        return tensor\n",
    "\n",
    "def trunc_normal_(tensor, mean=0.0, std=1.0, a=-2.0, b=2.0):\n",
    "    # type: (Tensor, float, float, float, float) -> Tensor\n",
    "    return _no_grad_trunc_normal_(tensor, mean, std, a, b)\n",
    "\n",
    "class PatchEmbedPerChannel(nn.Module):\n",
    "    \"\"\"Image to Patch Embedding.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_size: int = 224,\n",
    "        patch_size: int = 16,\n",
    "        in_chans: int = 3,\n",
    "        embed_dim: int = 768,\n",
    "        enable_sample: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        num_patches = (img_size // patch_size) * (img_size // patch_size) * in_chans\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "\n",
    "        self.proj = nn.Conv3d(\n",
    "            1,\n",
    "            embed_dim,\n",
    "            kernel_size=(1, patch_size, patch_size),\n",
    "            stride=(1, patch_size, patch_size),\n",
    "        )  # CHANGED\n",
    "\n",
    "        self.channel_embed = nn.Embedding(in_chans, embed_dim)\n",
    "        self.enable_sample = enable_sample\n",
    "\n",
    "        trunc_normal_(self.channel_embed.weight, std=0.02)\n",
    "\n",
    "    def forward(self, x, extra_tokens={}):\n",
    "        # # assume all images in the same batch has the same input channels\n",
    "        # cur_channels = extra_tokens[\"channels\"][0]\n",
    "        # embedding lookup\n",
    "        cur_channel_embed = self.channel_embed(\n",
    "            extra_tokens[\"channels\"]\n",
    "        )  # B, Cin, embed_dim=Cout\n",
    "        cur_channel_embed = cur_channel_embed.permute(0, 2, 1)  # B Cout Cin\n",
    "\n",
    "        B, Cin, H, W = x.shape\n",
    "        # Note: The current number of channels (Cin) can be smaller or equal to in_chans\n",
    "\n",
    "        if self.training and self.enable_sample:\n",
    "            # Per batch channel sampling\n",
    "            # Note this may be slow\n",
    "            # Randomly sample the number of channels for this batch\n",
    "            Cin_new = random.randint(1, Cin)\n",
    "\n",
    "            # Randomly sample the selected channels\n",
    "            channels = random.sample(range(Cin), k=Cin_new)\n",
    "            \n",
    "            Cin = Cin_new\n",
    "            x = x[:, channels, :, :]\n",
    "\n",
    "            # Update the embedding lookup\n",
    "            cur_channel_embed = cur_channel_embed[:, :, channels]\n",
    "            ######\n",
    "\n",
    "        # shared projection layer across channels\n",
    "        x = self.proj(x.unsqueeze(1))  # B Cout Cin H W\n",
    "\n",
    "        # channel specific offsets\n",
    "        x += cur_channel_embed.unsqueeze(-1).unsqueeze(-1)\n",
    "        # x += self.channel_embed[:, :, cur_channels, :, :]  # B Cout Cin H W\n",
    "\n",
    "        # preparing the output sequence\n",
    "        x = x.flatten(2)  # B Cout CinHW\n",
    "        x = x.transpose(1, 2)  # B CinHW Cout\n",
    "\n",
    "        return x, Cin\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        num_heads=8,\n",
    "        qkv_bias=False,\n",
    "        qk_scale=None,\n",
    "        attn_drop=0.0,\n",
    "        proj_drop=0.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = (\n",
    "            self.qkv(x)\n",
    "            .reshape(B, N, 3, self.num_heads, C // self.num_heads)\n",
    "            .permute(2, 0, 3, 1, 4)\n",
    "        )\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x, attn\n",
    "    \n",
    "class Mlp(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features,\n",
    "        hidden_features=None,\n",
    "        out_features=None,\n",
    "        act_layer=nn.GELU,\n",
    "        drop=0.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x    \n",
    "\n",
    "def drop_path(x, drop_prob: float = 0.0, training: bool = False):\n",
    "    if drop_prob == 0.0 or not training:\n",
    "        return x\n",
    "    keep_prob = 1 - drop_prob\n",
    "    shape = (x.shape[0],) + (1,) * (\n",
    "        x.ndim - 1\n",
    "    )  # work with diff dim tensors, not just 2D ConvNets\n",
    "    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
    "    random_tensor.floor_()  # binarize\n",
    "    output = x.div(keep_prob) * random_tensor\n",
    "    return output\n",
    "\n",
    "class DropPath(nn.Module):\n",
    "    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\"\"\"\n",
    "\n",
    "    def __init__(self, drop_prob=None):\n",
    "        super(DropPath, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        return drop_path(x, self.drop_prob, self.training)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        num_heads,\n",
    "        mlp_ratio=4.0,\n",
    "        qkv_bias=False,\n",
    "        qk_scale=None,\n",
    "        drop=0.0,\n",
    "        attn_drop=0.0,\n",
    "        drop_path=0.0,\n",
    "        act_layer=nn.GELU,\n",
    "        norm_layer=nn.LayerNorm,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = Attention(\n",
    "            dim,\n",
    "            num_heads=num_heads,\n",
    "            qkv_bias=qkv_bias,\n",
    "            qk_scale=qk_scale,\n",
    "            attn_drop=attn_drop,\n",
    "            proj_drop=drop,\n",
    "        )\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(\n",
    "            in_features=dim,\n",
    "            hidden_features=mlp_hidden_dim,\n",
    "            act_layer=act_layer,\n",
    "            drop=drop,\n",
    "        )\n",
    "\n",
    "    def forward(self, x, return_attention=False):\n",
    "        y, attn = self.attn(self.norm1(x))\n",
    "        if return_attention:\n",
    "            return attn\n",
    "        x = x + self.drop_path(y)\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        return x\n",
    "\n",
    "class ChannelVisionTransformer(nn.Module):\n",
    "    \"\"\"Channel Vision Transformer\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_size=[224],\n",
    "        patch_size=16,\n",
    "        in_chans=3,\n",
    "        num_classes=0,\n",
    "        embed_dim=768,\n",
    "        depth=12,\n",
    "        num_heads=12,\n",
    "        mlp_ratio=4.0,\n",
    "        qkv_bias=False,\n",
    "        qk_scale=None,\n",
    "        drop_rate=0.0,\n",
    "        attn_drop_rate=0.0,\n",
    "        drop_path_rate=0.0,\n",
    "        norm_layer=nn.LayerNorm,\n",
    "        enable_sample=True,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        print(\n",
    "            \"Warning!!!\\n\"\n",
    "            \"Samplev2 channel vit randomly sample channels for each batch.\\n\"\n",
    "            \"It is only compatible with Supervised learning\\n\"\n",
    "            \"Doesn't work with DINO or Linear Prob\"\n",
    "        )\n",
    "\n",
    "        self.num_features = self.embed_dim = self.out_dim = embed_dim\n",
    "        self.in_chans = in_chans\n",
    "        \n",
    "        if r_method == 'pca':\n",
    "            self.input_processing = PCAReduce(reduce_dim,spectral_means,spectral_loadings)\n",
    "        elif r_method == 'fixed':\n",
    "            self.input_processing = FixedReduction(input_dim=len(wavenumbers_used))\n",
    "        elif r_method == 'linear':\n",
    "            self.input_processing = LinearReduction(input_dim=len(wavenumbers_used),reduce_dim=reduce_dim)\n",
    "\n",
    "        self.patch_embed = PatchEmbedPerChannel(\n",
    "            img_size=img_size[0],\n",
    "            patch_size=patch_size,\n",
    "            in_chans=in_chans,\n",
    "            embed_dim=embed_dim,\n",
    "            enable_sample=enable_sample,\n",
    "        )\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "\n",
    "        self.num_extra_tokens = 1  # cls token\n",
    "\n",
    "        self.pos_embed = nn.Parameter(\n",
    "            torch.zeros(\n",
    "                1, num_patches // self.in_chans + self.num_extra_tokens, embed_dim\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        dpr = [\n",
    "            x.item() for x in torch.linspace(0, drop_path_rate, depth)\n",
    "        ]  # stochastic depth decay rule\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [\n",
    "                Block(\n",
    "                    dim=embed_dim,\n",
    "                    num_heads=num_heads,\n",
    "                    mlp_ratio=mlp_ratio,\n",
    "                    qkv_bias=qkv_bias,\n",
    "                    qk_scale=qk_scale,\n",
    "                    drop=drop_rate,\n",
    "                    attn_drop=attn_drop_rate,\n",
    "                    drop_path=dpr[i],\n",
    "                    norm_layer=norm_layer,\n",
    "                )\n",
    "                for i in range(depth)\n",
    "            ]\n",
    "        )\n",
    "        self.norm = norm_layer(embed_dim)\n",
    "\n",
    "        # Classifier head\n",
    "        self.head = (\n",
    "            nn.Linear(embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
    "        )\n",
    "\n",
    "        trunc_normal_(self.pos_embed, std=0.02)\n",
    "        trunc_normal_(self.cls_token, std=0.02)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=0.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    def interpolate_pos_encoding(self, x, w, h, c):\n",
    "        # number of auxilary dimensions before the patches\n",
    "        if not hasattr(self, \"num_extra_tokens\"):\n",
    "            # backward compatibility\n",
    "            num_extra_tokens = 1\n",
    "        else:\n",
    "            num_extra_tokens = self.num_extra_tokens\n",
    "\n",
    "        npatch = x.shape[1] - num_extra_tokens\n",
    "        N = self.pos_embed.shape[1] - num_extra_tokens\n",
    "\n",
    "        if npatch == N and w == h:\n",
    "            return self.pos_embed\n",
    "\n",
    "        class_pos_embed = self.pos_embed[:, :num_extra_tokens]\n",
    "        patch_pos_embed = self.pos_embed[:, num_extra_tokens:]\n",
    "\n",
    "        dim = x.shape[-1]\n",
    "        w0 = w // self.patch_embed.patch_size\n",
    "        h0 = h // self.patch_embed.patch_size\n",
    "        # we add a small number to avoid floating point error in the interpolation\n",
    "        # see discussion at https://github.com/facebookresearch/dino/issues/8\n",
    "        w0, h0 = w0 + 0.1, h0 + 0.1\n",
    "        patch_pos_embed = nn.functional.interpolate(\n",
    "            patch_pos_embed.reshape(\n",
    "                1, int(math.sqrt(N)), int(math.sqrt(N)), dim\n",
    "            ).permute(0, 3, 1, 2),\n",
    "            scale_factor=(w0 / math.sqrt(N), h0 / math.sqrt(N)),\n",
    "            mode=\"bicubic\",\n",
    "        )\n",
    "        assert (\n",
    "            int(w0) == patch_pos_embed.shape[-2]\n",
    "            and int(h0) == patch_pos_embed.shape[-1]\n",
    "        )\n",
    "        patch_pos_embed = patch_pos_embed.permute(0, 2, 3, 1).view(1, 1, -1, dim)\n",
    "\n",
    "        # create copies of the positional embeddings for each channel\n",
    "        patch_pos_embed = patch_pos_embed.expand(1, c, -1, dim).reshape(1, -1, dim)\n",
    "\n",
    "        return torch.cat((class_pos_embed, patch_pos_embed), dim=1)\n",
    "\n",
    "    def prepare_tokens(self, x, extra_tokens):\n",
    "        B, nc, w, h = x.shape\n",
    "        x, nc = self.patch_embed(x, extra_tokens)  # patch linear embedding\n",
    "\n",
    "        # add the [CLS] token to the embed patch tokens\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "\n",
    "        # add positional encoding to each token\n",
    "        x = x + self.interpolate_pos_encoding(x, w, h, nc)\n",
    "\n",
    "        return self.pos_drop(x)\n",
    "\n",
    "    def forward(self, x, extra_tokens={}):\n",
    "        x = self.input_processing(x)\n",
    "        x = self.prepare_tokens(x, extra_tokens)\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        x = self.norm(x)\n",
    "        return self.head(x[:, 0])\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-04T15:31:02.373511300Z",
     "start_time": "2025-04-04T15:31:02.267854600Z"
    }
   },
   "id": "33535f38f1458959",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning!!!\n",
      "Samplev2 channel vit randomly sample channels for each batch.\n",
      "It is only compatible with Supervised learning\n",
      "Doesn't work with DINO or Linear Prob\n"
     ]
    }
   ],
   "source": [
    "model = ChannelVisionTransformer(\n",
    "    img_size=[patch_dim,],\n",
    "    patch_size=16,\n",
    "    in_chans=reduce_dim, # todo change if not using linear reduc\n",
    "    num_classes=n_classes,\n",
    "    embed_dim=192, # todo\n",
    "    depth=12,\n",
    "    num_heads=4,\n",
    "    mlp_ratio=4,\n",
    "    qkv_bias=True,\n",
    "    norm_layer=partial(nn.LayerNorm, eps=1e-6),\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "chn_tok = {\"channels\":torch.from_numpy(np.arange(0,reduce_dim)).unsqueeze(0).to(device)}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-04T15:31:02.424088100Z",
     "start_time": "2025-04-04T15:31:02.374539800Z"
    }
   },
   "id": "1956e51b755a02cd",
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training Loop"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4dfdaf2ad556fa46"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr,weight_decay=l2)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=500, threshold=0.01, cooldown=250)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-04T15:31:02.424088100Z",
     "start_time": "2025-04-04T15:31:02.420697500Z"
    }
   },
   "id": "50a59f2ddf1e3f0d",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "training_losses,validation_losses = [],[]\n",
    "training_accs,validation_accs = [],[]\n",
    "training_f1ms,validation_f1ms = [],[]\n",
    "training_f1s,validation_f1s = [],[]\n",
    "lr_decreases = []\n",
    "current_iters = 0\n",
    "best_val_f1 = 0\n",
    "best_val_iter = 0\n",
    "stop_training=False"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-04T15:31:02.429388Z",
     "start_time": "2025-04-04T15:31:02.425126300Z"
    }
   },
   "id": "51f82808428ba870",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ON ITER: 0, metrics for last 100 iters:\n",
      "TRAIN --- | Loss: 1.826 | OA: 0.1406 | F1M: 0.0423\n",
      "VAL ----- | Loss: 2.167 | OA: 0.1562 | F1M: 0.0450\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "    \u001B[1;31m[... skipping hidden 1 frame]\u001B[0m\n",
      "Cell \u001B[1;32mIn[12], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m current_iters \u001B[38;5;241m<\u001B[39m max_iters:\n\u001B[1;32m----> 2\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m (bidx, (data, label)) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(train_loader):\n\u001B[0;32m      3\u001B[0m         data \u001B[38;5;241m=\u001B[39m data\u001B[38;5;241m.\u001B[39mto(device); label \u001B[38;5;241m=\u001B[39m label\u001B[38;5;241m.\u001B[39mto(device)\n",
      "File \u001B[1;32m~\\AppData\\Local\\anaconda3\\envs\\phd-env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    700\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[1;32m--> 701\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_data()\n\u001B[0;32m    702\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "File \u001B[1;32m~\\AppData\\Local\\anaconda3\\envs\\phd-env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    756\u001B[0m index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m--> 757\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_fetcher\u001B[38;5;241m.\u001B[39mfetch(index)  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m    758\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n",
      "File \u001B[1;32m~\\AppData\\Local\\anaconda3\\envs\\phd-env\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[1;34m(self, possibly_batched_index)\u001B[0m\n\u001B[0;32m     51\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 52\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[0;32m     53\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "Cell \u001B[1;32mIn[5], line 53\u001B[0m, in \u001B[0;36mftir_patching_dataset.__getitem__\u001B[1;34m(self, idx)\u001B[0m\n\u001B[0;32m     51\u001B[0m \u001B[38;5;66;03m# get patch\u001B[39;00m\n\u001B[0;32m     52\u001B[0m patch \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mfrom_numpy(\n\u001B[1;32m---> 53\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhdf5_files[cidx][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mspectra\u001B[39m\u001B[38;5;124m'\u001B[39m][idx_u:idx_d,idx_l:idx_r,\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchannels],\n\u001B[0;32m     54\u001B[0m )\u001B[38;5;241m.\u001B[39mpermute(\u001B[38;5;241m2\u001B[39m,\u001B[38;5;241m0\u001B[39m,\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m     55\u001B[0m patch \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mfrom_numpy(\n\u001B[0;32m     56\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhdf5_files[cidx][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmask\u001B[39m\u001B[38;5;124m'\u001B[39m][idx_u:idx_d,idx_l:idx_r,],\n\u001B[0;32m     57\u001B[0m )\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m0\u001B[39m)\n",
      "File \u001B[1;32mh5py\\\\_objects.pyx:54\u001B[0m, in \u001B[0;36mh5py._objects.with_phil.wrapper\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mh5py\\\\_objects.pyx:55\u001B[0m, in \u001B[0;36mh5py._objects.with_phil.wrapper\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m~\\AppData\\Local\\anaconda3\\envs\\phd-env\\Lib\\site-packages\\h5py\\_hl\\dataset.py:781\u001B[0m, in \u001B[0;36mDataset.__getitem__\u001B[1;34m(self, args, new_dtype)\u001B[0m\n\u001B[0;32m    780\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 781\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fast_reader\u001B[38;5;241m.\u001B[39mread(args)\n\u001B[0;32m    782\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "File \u001B[1;32m~\\AppData\\Local\\anaconda3\\envs\\phd-env\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3598\u001B[0m, in \u001B[0;36mInteractiveShell.run_code\u001B[1;34m(self, code_obj, result, async_)\u001B[0m\n\u001B[0;32m   3596\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m:\n\u001B[0;32m   3597\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m result \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m-> 3598\u001B[0m         result\u001B[38;5;241m.\u001B[39merror_in_exec \u001B[38;5;241m=\u001B[39m sys\u001B[38;5;241m.\u001B[39mexc_info()[\u001B[38;5;241m1\u001B[39m]\n\u001B[0;32m   3599\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mshowtraceback(running_compiled_code\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m   3600\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "while current_iters < max_iters:\n",
    "    for (bidx, (data, label)) in enumerate(train_loader):\n",
    "        data = data.to(device); label = label.to(device)\n",
    "        \n",
    "        # Push through model\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data,extra_tokens=chn_tok)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = loss_fn(out, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Append log arrays\n",
    "        training_losses.append(loss.item())\n",
    "        pred = out.argmax(dim=1).detach().cpu().numpy()\n",
    "        actual = label.cpu().numpy()\n",
    "        training_accs.append(accuracy_score(actual,pred))\n",
    "        training_f1ms.append(f1_score(actual, pred, average='macro'))\n",
    "        training_f1s.append(f1_score(actual, pred, average=None, labels=np.arange(0,n_classes),zero_division=0))\n",
    "        \n",
    "        # Do validation cycle\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            # load data\n",
    "            data, label = next(iter(val_loader))\n",
    "            data = data.to(device); label = label.to(device)\n",
    "            \n",
    "            # Push through model\n",
    "            out = model(data,extra_tokens=chn_tok)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = loss_fn(out, label)\n",
    "            \n",
    "            # Append log arrays\n",
    "            validation_losses.append(loss.item())\n",
    "            pred = out.argmax(dim=1).detach().cpu().numpy()\n",
    "            actual = label.cpu().numpy()\n",
    "            validation_accs.append(accuracy_score(actual,pred))\n",
    "            validation_f1ms.append(f1_score(actual, pred, average='macro'))\n",
    "            validation_f1s.append(f1_score(actual, pred, average=None, labels=np.arange(0,n_classes),zero_division=0))\n",
    "        \n",
    "        # Print training statistics every N iters\n",
    "        if current_iters % pseudo_epoch == 0:\n",
    "            print(f\"ON ITER: {current_iters}, metrics for last {pseudo_epoch} iters:\")\n",
    "            print(f\"TRAIN --- | Loss: {np.mean(training_losses[-pseudo_epoch:]):.4} | OA: {np.mean(training_accs[-pseudo_epoch:]):.4} | F1M: {np.mean(training_f1ms[-pseudo_epoch:]):.4f}\")\n",
    "            print(f\"VAL ----- | Loss: {np.mean(validation_losses[-pseudo_epoch:]):.4} | OA: {np.mean(validation_accs[-pseudo_epoch:]):.4} | F1M: {np.mean(validation_f1ms[-pseudo_epoch:]):.4f}\")\n",
    "        \n",
    "        # If performance on validation set best so far, save model\n",
    "        if np.mean(validation_f1ms[-pseudo_epoch:]) > best_val_f1:\n",
    "            best_val_f1 = np.mean(validation_f1ms[-pseudo_epoch:])\n",
    "            best_val_iter = current_iters\n",
    "            if not is_local:\n",
    "                torch.save(model.state_dict(), rf'./model_weights_{seed}.pt')\n",
    "        \n",
    "        # Step the scheduler based on the validation set performance\n",
    "        current_iters += 1\n",
    "        if current_iters > max_iters: \n",
    "            stop_training = True\n",
    "            break\n",
    "        if current_iters > pseudo_epoch:\n",
    "            scheduler.step(np.mean(validation_f1ms[-pseudo_epoch:]))\n",
    "            new_lr = optimizer.param_groups[0]['lr']\n",
    "            if new_lr != lr:\n",
    "                print(f\"Val f1 plateaued, lr {lr} -> {new_lr}\")\n",
    "                lr = new_lr\n",
    "                lr_decreases.append(current_iters)\n",
    "                if len(lr_decreases) >= 2: \n",
    "                    stop_training = True\n",
    "                    print(\"Val f1 decreased twice, ending training early\")\n",
    "                    break\n",
    "    if stop_training: break\n",
    "\n",
    "training_losses = np.array(training_losses); validation_losses = np.array(validation_losses)\n",
    "training_accs = np.array(training_accs); validation_accs = np.array(validation_accs)\n",
    "training_f1ms = np.array(training_f1ms); validation_f1ms = np.array(validation_f1ms)\n",
    "training_f1s = np.stack(training_f1s,axis=0); validation_f1s = np.stack(validation_f1s,axis=0)\n",
    "print(f\"Training complete after {current_iters} iterations\\n\\ttotal samples       :    {current_iters*batch_size}\\n\\t -=-=-=-=-=-=-=-=-=-=-=-=-=-\")\n",
    "for cls_idx, samples_loaded in enumerate(dataset_train.total_sampled.numpy()):\n",
    "    print(f\"\\t{annotation_class_names[cls_idx]}{(20-len(annotation_class_names[cls_idx])) * ' '}:    {int(samples_loaded)}\")\n",
    "print(f\"Metrics for final {pseudo_epoch} iterations:\")\n",
    "print(f\"TRAIN --- | Loss: {training_losses[-pseudo_epoch:].mean():.4f} | OA: {training_accs[-pseudo_epoch:].mean():.4f} | f1: {training_f1ms[-pseudo_epoch:].mean():.4f}\")\n",
    "print(f\"VAL ----- | Loss: {validation_losses[-pseudo_epoch:].mean():.4f} | OA: {validation_accs[-pseudo_epoch:].mean():.4f} | f1: {validation_f1ms[-pseudo_epoch:].mean():.4f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-04T15:33:00.028630700Z",
     "start_time": "2025-04-04T15:31:02.433478400Z"
    }
   },
   "id": "7c6adbd14fd799f6",
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test Loop"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "307cf693fb06cb7e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "running_loss_test = 0\n",
    "test_preds, test_targets = [], []\n",
    "\n",
    "if not is_local:\n",
    "    model.load_state_dict(torch.load(rf'./model_weights_{seed}.pt', weights_only=True))\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (data, label) in enumerate(test_loader):\n",
    "        print(f\"Iter: {batch_idx}/{len(test_loader)}\",end=\"\\r\")        \n",
    "        data = data.to(device)\n",
    "        label = label.to(device)\n",
    "\n",
    "        # Push through model\n",
    "        out = model(data,extra_tokens=chn_tok)\n",
    "        loss = loss_fn(out, label)\n",
    "\n",
    "        # Calculate metrics\n",
    "        running_loss_test += loss.cpu().item() \n",
    "        pred = out.argmax(dim=1).detach().cpu().numpy()\n",
    "        actual = label.cpu().numpy()\n",
    "        test_preds.extend(pred)\n",
    "        test_targets.extend(actual)\n",
    "\n",
    "test_targets = np.array(test_targets); test_preds = np.array(test_preds)\n",
    "test_loss = running_loss_test / batch_idx\n",
    "test_acc = accuracy_score(test_targets, test_preds)\n",
    "test_f1m = f1_score(test_targets, test_preds, average='macro')\n",
    "test_f1 = f1_score(test_targets, test_preds, average=None)\n",
    "\n",
    "print(\"Metrics on entire testing set:\")\n",
    "print(f\"TEST ---- | Loss: {test_loss:.4f} | OA: {test_acc:.4f} | f1: {test_f1m:.4f}\")\n",
    "for cls_idx, f1 in enumerate(test_f1):\n",
    "    print(f\"{annotation_class_names[cls_idx]}{(20 - len(annotation_class_names[cls_idx])) * ' '} : {f1:.4f}\")\n",
    "print(\"Total samples loaded for each class during TESTING\")\n",
    "for cls_idx, samples_loaded in enumerate(dataset_test.total_sampled.numpy()):\n",
    "    print(f\"{annotation_class_names[cls_idx]}{(20-len(annotation_class_names[cls_idx])) * ' '}:    {int(samples_loaded)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-04-04T15:33:00.027604800Z"
    }
   },
   "id": "242a34c1a6fe3be8",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "67ff17a2ebecf4ae"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def moving_average(a, n=3): # https://stackoverflow.com/questions/14313510/how-to-calculate-rolling-moving-average-using-python-numpy-scipy\n",
    "    a = np.pad(a, ((n-1)//2,(n-1)//2 + ((n-1) % 2)), mode='edge')\n",
    "    ret = np.cumsum(a, dtype=float)\n",
    "    ret[n:] = ret[n:] - ret[:-n]\n",
    "    return ret[n - 1:] / n\n",
    "\n",
    "fig,ax = plt.subplots(1,3,figsize=(16,5))\n",
    "ax[0].plot(np.arange(0,len(moving_average(training_losses,n=1))),moving_average(training_losses,n=1),alpha=0.3,color='cornflowerblue')\n",
    "ax[0].plot(np.arange(0,len(moving_average(training_losses,n=50))),moving_average(training_losses,n=50),alpha=1,color='cornflowerblue',label=\"train\")\n",
    "ax[0].plot(np.arange(0,len(moving_average(validation_losses,n=1))),moving_average(validation_losses,n=1),alpha=0.3,color='orange')\n",
    "ax[0].plot(np.arange(0,len(moving_average(validation_losses,n=50))),moving_average(validation_losses,n=50),alpha=1,color='orange',label=\"validation\")\n",
    "ax[0].scatter(current_iters,test_loss,color='green',label=\"test\",marker=\"x\")\n",
    "ax[0].set_title(\"Loss\"); ax[0].legend()\n",
    "\n",
    "ax[1].plot(np.arange(0,len(moving_average(training_accs,n=1))),moving_average(training_accs,n=1),alpha=0.3,color='cornflowerblue')\n",
    "ax[1].plot(np.arange(0,len(moving_average(training_accs,n=50))),moving_average(training_accs,n=50),alpha=1,color='cornflowerblue',label=\"train\")\n",
    "ax[1].plot(np.arange(0,len(moving_average(validation_accs,n=1))),moving_average(validation_accs,n=1),alpha=0.3,color='orange')\n",
    "ax[1].plot(np.arange(0,len(moving_average(validation_accs,n=50))),moving_average(validation_accs,n=50),alpha=1,color='orange',label=\"validation\")\n",
    "ax[1].scatter(current_iters,test_acc,color='green',label=\"test\",marker=\"x\")\n",
    "ax[1].set_title(\"Accuracy\"); ax[1].legend()\n",
    "\n",
    "ax[2].plot(np.arange(0,len(moving_average(training_f1ms,n=1))),moving_average(training_f1ms,n=1),alpha=0.3,color='cornflowerblue')\n",
    "ax[2].plot(np.arange(0,len(moving_average(training_f1ms,n=50))),moving_average(training_f1ms,n=50),alpha=1,color='cornflowerblue',label=\"train\")\n",
    "ax[2].plot(np.arange(0,len(moving_average(validation_f1ms,n=1))),moving_average(validation_f1ms,n=1),alpha=0.3,color='orange')\n",
    "ax[2].plot(np.arange(0,len(moving_average(validation_f1ms,n=50))),moving_average(validation_f1ms,n=50),alpha=1,color='orange',label=\"validation\")\n",
    "ax[2].scatter(current_iters,test_f1m,color='green',label=\"test\",marker=\"x\")\n",
    "ax[2].set_title(\"Macro F1 Score\"); ax[2].legend()\n",
    "\n",
    "ax[0].axvline(x=best_val_iter, ymin=0, ymax=1, color='red',alpha=0.3)\n",
    "ax[1].axvline(x=best_val_iter, ymin=0, ymax=1, color='red',alpha=0.3)\n",
    "ax[2].axvline(x=best_val_iter, ymin=0, ymax=1, color='red',alpha=0.3)\n",
    "\n",
    "for lrd in lr_decreases:\n",
    "    ax[0].axvline(x=lrd, ymin=0, ymax=1, color='grey',alpha=0.3)\n",
    "    ax[1].axvline(x=lrd, ymin=0, ymax=1, color='grey',alpha=0.3)\n",
    "    ax[2].axvline(x=lrd, ymin=0, ymax=1, color='grey',alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "if not is_local:\n",
    "    plt.savefig(f'./loss_curve_{seed}.png'); plt.close(fig)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-04-04T15:33:00.027604800Z"
    }
   },
   "id": "5aa6f4abb81d43a8",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "training_f1s = np.stack(training_f1s,axis=0)\n",
    "validation_f1s = np.stack(validation_f1s,axis=0)\n",
    "fig,ax = plt.subplots(2,3,figsize=(15,5)); ax = ax.flatten()\n",
    "for cls in range(n_classes):\n",
    "    ax[cls].plot(np.arange(0,len(moving_average(training_f1s[:,cls],n=1))),moving_average(training_f1s[:,cls],n=1),alpha=0.3,color='k')\n",
    "    ax[cls].plot(np.arange(0,len(moving_average(training_f1s[:,cls],n=50))),moving_average(training_f1s[:,cls],n=50),alpha=1,color='k',label=\"train\")\n",
    "    ax[cls].plot(np.arange(0,len(moving_average(validation_f1s[:,cls],n=1))),moving_average(validation_f1s[:,cls],n=1),alpha=0.3,color=annotation_class_colors[cls]/255)\n",
    "    ax[cls].plot(np.arange(0,len(moving_average(validation_f1s[:,cls],n=50))),moving_average(validation_f1s[:,cls],n=50),alpha=1,color=annotation_class_colors[cls]/255, label=\"validation\")\n",
    "    ax[cls].scatter(current_iters,test_f1[cls],color=annotation_class_colors[cls]/255,label=\"test\",marker=\"x\")\n",
    "    ax[cls].set_ylim(ymin=0,ymax=1)\n",
    "    for lrd in lr_decreases:\n",
    "        ax[cls].axvline(x=lrd, ymin=0, ymax=1, color='grey',alpha=0.5)\n",
    "    ax[cls].axvline(x=best_val_iter, ymin=0, ymax=1, color='red',alpha=0.3)\n",
    "fig.suptitle(\"Class-specific F1 scores\")\n",
    "plt.tight_layout()\n",
    "if not is_local:\n",
    "    plt.savefig(f'./loss_curve_individual_{seed}.png'); plt.close(fig)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-04T15:33:00.031651600Z",
     "start_time": "2025-04-04T15:33:00.029627700Z"
    }
   },
   "id": "60c4c1d2914ee636",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Finish experiment"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5cd7e4042615b52"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "if not is_local:\n",
    "    model = model.cpu()\n",
    "    torch.save(model.state_dict(), rf'./model_weights_{seed}.pt')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-04-04T15:33:00.030654800Z"
    }
   },
   "id": "b8248f10250d5d36"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Read existing results file\n",
    "if not is_local:\n",
    "    if os.path.isfile('results.txt'):\n",
    "        f = open('results.txt','r')\n",
    "        lines = f.readlines()\n",
    "        f.close()\n",
    "    else: \n",
    "        lines = [x+', \\n' for x in['seed',*annotation_class_names,'overall_acc','macro_f1']]\n",
    "        \n",
    "    # Process files\n",
    "    lines[0] = lines[0].replace('\\n',str(seed) + ', \\n')\n",
    "    for cls in range(n_classes):\n",
    "        lines[cls+1] = lines[cls+1].replace('\\n',str(test_f1[cls]) + ', \\n' )\n",
    "    lines[n_classes+1] = lines[n_classes+1].replace('\\n',str(test_acc) + ', \\n')\n",
    "    lines[n_classes+2] = lines[n_classes+2].replace('\\n',str(test_f1m) + ', \\n')\n",
    "    \n",
    "    f = open('results.txt','w')\n",
    "    f.write(''.join(lines))\n",
    "    f.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-04T15:33:00.033673700Z",
     "start_time": "2025-04-04T15:33:00.031651600Z"
    }
   },
   "id": "bcb3c45e041764e8",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([16, 192])"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.patch_embed.channel_embed.weight.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-04T15:38:50.420861Z",
     "start_time": "2025-04-04T15:38:50.411637600Z"
    }
   },
   "id": "bf1037863d7a149b",
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "cbedfd1bbaae06a8"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 1200x600 with 2 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig,ax = plt.subplots(1,2,figsize=(12,6))\n",
    "locarray = np.zeros((model.pos_embed.shape[1]-1,model.pos_embed.shape[1]-1))\n",
    "cos = nn.CosineSimilarity(dim=0)\n",
    "for r in range(model.pos_embed.shape[1]-1):\n",
    "    for c in range(model.pos_embed.shape[1]-1):\n",
    "        locarray[r,c] = cos(model.pos_embed[0][r + 1],model.pos_embed[0][c + 1]).detach().cpu().numpy()\n",
    "chnarray = np.zeros((reduce_dim,reduce_dim))\n",
    "for r in range(reduce_dim):\n",
    "    for c in range(reduce_dim):\n",
    "        chnarray[r,c] = cos(model.patch_embed.channel_embed.weight[r],model.patch_embed.channel_embed.weight[c]).detach().cpu().numpy()\n",
    "ax[0].set_title(\"position embeddings\"); ax[1].set_title(\"channel embeddings\")\n",
    "ax[0].matshow(locarray); ax[1].matshow(chnarray)\n",
    "fig.tight_layout()\n",
    "if not is_local:\n",
    "    plt.savefig(f'./position_embedding_{seed}.png'); plt.close(fig) "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-04T15:41:00.550100100Z",
     "start_time": "2025-04-04T15:41:00.238496400Z"
    }
   },
   "id": "a03c4474769e5d5f",
   "execution_count": 25
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
